{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Plan\n",
    "********\n",
    "\n",
    "# Data Exploration and feature identification\n",
    "\n",
    "> Events in a patient's therapy can be broadly categorized into -\n",
    "###  Diagnosis/Surgery events\n",
    "        * How frequently is a patient diagnosed? What is the average gap between Dx for a patient?\n",
    "        * How much does a patient spend on Dx?\n",
    "        * What kind of surgeries involve opioid therapy?\n",
    "        * ...\n",
    "###  Claims/prescription events\n",
    "        * How frequently a patient on opioid therapy takes opioid claims?\n",
    "        * How much does a patient spends on opioid claims? Actual and OOP cost both...\n",
    "        * Who are the payors in market? How's HUMANA doing in the market? Any close competitors?\n",
    "        * What is the rejection ratio in the market? Across Payors?\n",
    "        * What physician segments are prescribing opioids? Any specialization required to prescribe opioids?\n",
    "        * ...\n",
    "###  Promotional events\n",
    "        * What do these calls mean, Mbr? Prov? Other?\n",
    "        * How frequently a patient is being reached through these calls?\n",
    "        * Are LTOT patients reached more frequently through these calls?\n",
    "        * ...\n",
    "        \n",
    "# Feature selection\n",
    "> Idea is to create a patient level feature matrix which will be used to predict LTOT patients. Features identified in previous stage can be used to build the feature matrix...\n",
    "\n",
    "# Model fitting\n",
    "> Classification problem\n",
    ">> Logistic regression, KNN, Trees, Ensemble methods...\n",
    "\n",
    "# Model validation and final results\n",
    "> Based on model accuracy...\n",
    "\n",
    "********\n",
    "********\n",
    "\n",
    "\n",
    "# Data related questions\n",
    "******\n",
    "> * What is the definition of 'on hand'? Any identifier in data to identify opioid medication from other medication?\n",
    "#####  If a patient is taking opioids, he is 'on-hand'. \"Rx Claim - Paid\" with a valid MME defines an opioid Rx claim. \n",
    "> * What is the market basket for the given data? What all drugs other then opioids are included in the given data? \n",
    "Is entire diagnosis history of a patient who has taken opioid in the analysis timeframe available in the data?\n",
    "##### Kind of Yes, the data captures events that are not related to opioids too.\n",
    "> * What is difference between Rx cost, Net Paid amount (attr 4), Member responsible amount (attr 9) and Payble quantity? --to calculate cost related KPIs\n",
    "##### Rx cost = Total cost ; Net paid amount = Amount paid by HUMANA (payor) = (Total cost - OOP)\n",
    "> * What is 'New diagnosis top 5'?\n",
    "##### Something regarding the other Dx patient went through apart from the ones listed. Not so important\n",
    "> * In case of multiple initiation events for a patient, which event is reference point for #days given in 'days' column?\n",
    "##### Initiation event  = Opioid Naive claim with a valid MME\n",
    "##### Opiod Naive claim = No Opioid taken in 90 days lookback\n",
    "> * what is the difference between event 'Fully Paid Claim' and 'RX Claim-Paid'?\n",
    "##### Fully paid claim are medical claims where as Rx claim paid are pharmacy claims\n",
    "> * Definition of LTOT - \n",
    "##### Patient >=162 days ‘on hand’ within 180 days after qualifying claim/Initiation event\n",
    "##### A patient needs to be tagged as LTOT or not after every initiation event i.e. everytime a patient is decleared a new opioid patient, he/she needs to be tagged as LTOT or not.\n",
    "##### Patient can have more than 1 initiation event which means patient has returned to the therapy and considered as new opioid patient. We have to predict about every new patients whether they will be on LTOT or not.\n",
    "> * ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import nbformat\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(s):\n",
    "    return str(s)\n",
    "\n",
    "def to_float(s):\n",
    "    return float(s)\n",
    "\n",
    "def tx_cat(s):\n",
    "    if s in('PAIN','OTH-STEROIDS','PSYCH-ANX','PSYCH-DEP','NUISANCE-SLEEP','PSYCH','NUISANCE-STIMULANTS',\n",
    "            'CARDIO', 'CANCER', 'OTH-THYROID','OTH-GROWTH HORMONE'):\n",
    "        return 'Pain Treatment'\n",
    "    else:\n",
    "        return 'Other Treatment'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'event_descr', 'event_attr1', 'event_attr2', 'event_attr3',\n",
      "       'event_attr4', 'event_attr5', 'event_attr6', 'event_attr7',\n",
      "       'event_attr8', 'event_attr9', 'event_attr10', 'PAY_DAY_SUPPLY_CNT',\n",
      "       'PAYABLE_QTY', 'MME', 'DRUG_TYPE', 'Specialty', 'Specialty2',\n",
      "       'Specialty3', 'Days'],\n",
      "      dtype='object')\n",
      "1480394\n"
     ]
    }
   ],
   "source": [
    "# zf = zipfile.ZipFile('/Users/udbhavverma/Documents/Humana_case_2019/data/Download.zip')\n",
    "# zf.printdir()\n",
    "\n",
    "data_train = pd.read_csv('/Users/udbhavverma/Documents/Fall\\'19/Data_HUMANA/Download/HMAHCC_COMP.csv')\n",
    "data_ho = pd.read_csv('/Users/udbhavverma/Documents/Fall\\'19/Data_HUMANA/Download/HMAHCC_HOLDOUT.csv')\n",
    "\n",
    "data_ho.rename(columns={'ID':'id', 'EVENT_DESCR':'event_descr', 'EVENT_ATTR1':'event_attr1',\n",
    "                           'EVENT_ATTR2':'event_attr2', 'EVENT_ATTR3':'event_attr3', \n",
    "                           'EVENT_ATTR4':'event_attr4', 'EVENT_ATTR5':'event_attr5', \n",
    "                           'EVENT_ATTR6':'event_attr6', 'EVENT_ATTR7':'event_attr7',\n",
    "                           'EVENT_ATTR8':'event_attr8', 'EVENT_ATTR9':'event_attr9', \n",
    "                           'EVENT_ATTR10':'event_attr10', 'SPECIALTY':'Specialty', \n",
    "                           'SPECIALTY2':'Specialty2','SPECIALTY3':'Specialty3', 'days':'Days'},inplace=True)\n",
    "print (data_ho.columns)\n",
    "# data_train[:5]\n",
    "print (len(data_ho))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RxClaim      3690174\n",
      "Claim        1766863\n",
      "Call          442599\n",
      "Provider      171288\n",
      "Diagnosis      16045\n",
      "Name: event_category, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1480394"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Creating event categories\n",
    "\n",
    "def event_cat(s):\n",
    "        if s[0:7] == 'Inbound':\n",
    "            return 'Call'\n",
    "        elif s[0:13] =='New diagnosis':\n",
    "            return 'Diagnosis'\n",
    "        elif s[0:2] == 'RX':\n",
    "            return 'RxClaim'\n",
    "        elif s =='New provider':\n",
    "            return 'Provider'\n",
    "        else:\n",
    "            return 'Claim'\n",
    "        \n",
    "data_train['event_category'] = data_train['event_descr'].map(event_cat)   \n",
    "data_ho['event_category'] = data_ho['event_descr'].map(event_cat)\n",
    "\n",
    "print (data_train['event_category'].value_counts())\n",
    "data_train[['event_category','event_descr']].drop_duplicates()\n",
    "len(data_ho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout set processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are patients with more than 1 day=0 events\n",
      "No of opioid claims taken on day0 and count of patient\n",
      "1    5800\n",
      "2     132\n",
      "0      67\n",
      "3       1\n",
      "Name: sum, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28167, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>episode_num</th>\n",
       "      <th>episode_start_day</th>\n",
       "      <th>episode_end_day</th>\n",
       "      <th>actual_dos</th>\n",
       "      <th>PAY_DAY_SUPPLY_CNT</th>\n",
       "      <th>PAYABLE_QTY</th>\n",
       "      <th>total_rx_MME</th>\n",
       "      <th>rx_cost</th>\n",
       "      <th>mem_res_cost</th>\n",
       "      <th>prescribed_MME_per_day</th>\n",
       "      <th>consumed_MME_per_day</th>\n",
       "      <th>in_next_180_days</th>\n",
       "      <th>cutoff_180_day</th>\n",
       "      <th>days_on_op_next_180</th>\n",
       "      <th>LTOT_flag</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID10006701904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-331</td>\n",
       "      <td>-271.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>12.99</td>\n",
       "      <td>2.65</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID10006701904</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-245</td>\n",
       "      <td>-138.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>25.98</td>\n",
       "      <td>5.30</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>50.467290</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID10006701904</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>12.99</td>\n",
       "      <td>2.95</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID10020514442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-549</td>\n",
       "      <td>-542.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>7.54</td>\n",
       "      <td>7.54</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID10020514442</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-540</td>\n",
       "      <td>-530.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>14.90</td>\n",
       "      <td>14.90</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID10020514442</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.95</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID10025902702</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>6.20</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID10028172382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-899</td>\n",
       "      <td>-857.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2167.5</td>\n",
       "      <td>19.80</td>\n",
       "      <td>19.80</td>\n",
       "      <td>49.261364</td>\n",
       "      <td>51.607143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID10028172382</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-815</td>\n",
       "      <td>-800.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.79</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID10028172382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-795</td>\n",
       "      <td>-785.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  episode_num  episode_start_day  episode_end_day  actual_dos  \\\n",
       "0  ID10006701904          1.0               -331           -271.0        60.0   \n",
       "1  ID10006701904          2.0               -245           -138.0       107.0   \n",
       "2  ID10006701904          3.0                  0             60.0        60.0   \n",
       "3  ID10020514442          1.0               -549           -542.0         7.0   \n",
       "4  ID10020514442          2.0               -540           -530.0        10.0   \n",
       "5  ID10020514442          3.0                  0             15.0        15.0   \n",
       "6  ID10025902702          1.0                  0             10.0        10.0   \n",
       "7  ID10028172382          1.0               -899           -857.0        42.0   \n",
       "8  ID10028172382          2.0               -815           -800.0        15.0   \n",
       "9  ID10028172382          3.0               -795           -785.0        10.0   \n",
       "\n",
       "   PAY_DAY_SUPPLY_CNT  PAYABLE_QTY  total_rx_MME  rx_cost  mem_res_cost  \\\n",
       "0                60.0        180.0        2700.0    12.99          2.65   \n",
       "1               120.0        360.0        5400.0    25.98          5.30   \n",
       "2                60.0        180.0        2700.0    12.99          2.95   \n",
       "3                 7.0         28.0         560.0     7.54          7.54   \n",
       "4                10.0         60.0        1800.0    14.90         14.90   \n",
       "5                15.0         15.0          75.0     2.95          2.95   \n",
       "6                10.0         40.0         800.0     6.20          6.20   \n",
       "7                44.0        114.0        2167.5    19.80         19.80   \n",
       "8                15.0         30.0         270.0     7.79          7.79   \n",
       "9                10.0         60.0        1800.0     5.00          5.00   \n",
       "\n",
       "   prescribed_MME_per_day  consumed_MME_per_day  in_next_180_days  \\\n",
       "0               45.000000             45.000000                 0   \n",
       "1               45.000000             50.467290                 0   \n",
       "2               45.000000             45.000000                 1   \n",
       "3               80.000000             80.000000                 0   \n",
       "4              180.000000            180.000000                 0   \n",
       "5                5.000000              5.000000                 1   \n",
       "6               80.000000             80.000000                 1   \n",
       "7               49.261364             51.607143                 0   \n",
       "8               18.000000             18.000000                 0   \n",
       "9              180.000000            180.000000                 0   \n",
       "\n",
       "   cutoff_180_day  days_on_op_next_180  LTOT_flag category  \n",
       "0             0.0                 60.0        0.0      pre  \n",
       "1             0.0                 60.0        0.0      pre  \n",
       "2            60.0                 60.0        0.0     post  \n",
       "3             0.0                 15.0        0.0      pre  \n",
       "4             0.0                 15.0        0.0      pre  \n",
       "5            15.0                 15.0        0.0     post  \n",
       "6            10.0                 10.0        0.0     post  \n",
       "7             0.0                 90.0        0.0      pre  \n",
       "8             0.0                 90.0        0.0      pre  \n",
       "9             0.0                 90.0        0.0      pre  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ##Patients with >1 initiation (days=0)\n",
    "rx = data_ho[(data_ho['event_category']=='RxClaim')]#[:1000]\n",
    "rx.shape\n",
    "rx_paid_claim = rx[(rx['event_descr']=='RX Claim - Paid') & (rx['Days']==0)]\n",
    "rx_paid_claim_pat = rx_paid_claim.groupby(['id'])[['event_descr']].count().rename(columns={'event_descr':'count'})\n",
    "rx_paid_claim_pat['count'].value_counts().plot(kind='bar')\n",
    "xlabel('#Initiation claims')\n",
    "ylabel('#Patients')\n",
    "show()\n",
    "print ('There are patients with more than 1 day=0 events') # ##ID10081072715 :3 claims on day 0\n",
    "\n",
    "# ## Naive Opioid claims\n",
    "rx['Naive_opioid_claim'] = np.where((rx['event_descr']=='RX Claim - Paid')& (rx['Days']==0) & (~rx['MME'].isnull()),1,0)\n",
    "rx_naive_claims_pat = rx.groupby(['id'])[['Naive_opioid_claim']].sum().rename(columns={'Naive_opioid_claim':'sum'})\n",
    "print ('No of opioid claims taken on day0 and count of patient')\n",
    "print (rx_naive_claims_pat['sum'].value_counts())\n",
    "\n",
    "\n",
    "# ##Opioid Episode level summary with LTOT flag\n",
    "rx_op_claim = rx[(rx['event_descr']=='RX Claim - Paid')&(~rx['MME'].isnull())].sort_values(['id','Days'])\n",
    "rx_op_claim['total_rx_MME'] = rx_op_claim['MME']*rx_op_claim['PAYABLE_QTY']\n",
    "rx_op_claim['rx_cost'] = rx_op_claim['event_attr3'].map(to_float)\n",
    "rx_op_claim['mem_res_cost'] = rx_op_claim['event_attr9'].map(to_float)\n",
    "rx_op_claim = rx_op_claim.groupby(['id','Days'])[['rx_cost','mem_res_cost','PAY_DAY_SUPPLY_CNT','PAYABLE_QTY','total_rx_MME']].sum()\n",
    "rx_op_claim.reset_index(inplace=True)\n",
    "rx_op_claim['MME'] = 1*rx_op_claim['total_rx_MME']/rx_op_claim['PAY_DAY_SUPPLY_CNT']\n",
    "\n",
    "grace = 0\n",
    "# rx_op_claim = rx_op_claim[rx_op_claim['id']=='ID10010854159']\n",
    "# ID10010854159\n",
    "# ID10433575841\n",
    "rx_op_claim['claim_end_day'] = rx_op_claim['Days']+rx_op_claim['PAY_DAY_SUPPLY_CNT']\n",
    "rx_op_claim['next_op'] = rx_op_claim.groupby(\"id\")['Days'].shift(-1) \n",
    "rx_op_claim['actual_dos'] = np.where((rx_op_claim['claim_end_day']<rx_op_claim['next_op'])|(rx_op_claim['next_op'].isnull()),rx_op_claim['PAY_DAY_SUPPLY_CNT'],rx_op_claim['next_op']-rx_op_claim['Days'])\n",
    "rx_op_claim['gap_to_next_op'] = rx_op_claim['next_op']-rx_op_claim['claim_end_day']\n",
    "rx_op_claim['previous_op_end_day'] = rx_op_claim.groupby(\"id\")['claim_end_day'].shift(1) \n",
    "rx_op_claim['gap_from_previous_op'] = rx_op_claim['Days']-rx_op_claim['previous_op_end_day']\n",
    "rx_op_claim['new_patient_flag'] = np.where((rx_op_claim['gap_from_previous_op'].isnull())|(rx_op_claim['gap_from_previous_op']>90),1,0)\n",
    "\n",
    "# rx_op_claim.groupby('id')['new_patient_flag'].sum().value_counts().plot(kind='bar')\n",
    "\n",
    "rx_op_claim['is_episode_start'] = np.where((rx_op_claim['gap_from_previous_op']>grace)|(rx_op_claim['gap_from_previous_op'].isnull()),1,0)\n",
    "rx_op_claim['is_episode_end'] = np.where((rx_op_claim['next_op']>rx_op_claim['claim_end_day']+grace)|(rx_op_claim['gap_to_next_op'].isnull()),1,0)\n",
    "\n",
    "rx_op_claim['episode_start_day'] = np.where((rx_op_claim['gap_from_previous_op']>grace)|(rx_op_claim['gap_from_previous_op'].isnull()),rx_op_claim['Days'],999999999)\n",
    "rx_op_claim['episode_end_day'] = np.where((rx_op_claim['next_op']>rx_op_claim['claim_end_day']+grace)|(rx_op_claim['gap_to_next_op'].isnull()),rx_op_claim['claim_end_day'],-99999999)\n",
    "rx_op_claim['episode_rank'] = rx_op_claim.groupby(['id','is_episode_start'])['episode_start_day'].rank(method='dense')\n",
    "\n",
    "# rx_op_claim\n",
    "rx_op_claim.reset_index(inplace=True)\n",
    "for index, row in rx_op_claim.iterrows():      \n",
    "    if int(index)==0:\n",
    "        rx_op_claim.loc[index,'episode_num'] = 1\n",
    "    else:\n",
    "        if (rx_op_claim.loc[index,'is_episode_start']==0)&(rx_op_claim.loc[index,'episode_rank']==1):\n",
    "            rx_op_claim.loc[index,'episode_num'] = rx_op_claim.loc[int(index)-1,'episode_num']\n",
    "        else:\n",
    "            rx_op_claim.loc[index,'episode_num'] = rx_op_claim.loc[index,'episode_rank']\n",
    "\n",
    "aggregation = {'episode_start_day':min,'episode_end_day':max,'actual_dos':sum,\n",
    "               'PAY_DAY_SUPPLY_CNT':sum,'PAYABLE_QTY':sum,'total_rx_MME':sum,\n",
    "               'rx_cost':sum,'mem_res_cost':sum}        \n",
    "rx_op_episode = rx_op_claim.groupby(['id','episode_num']).agg(aggregation)\n",
    "rx_op_episode['prescribed_MME_per_day'] = 1*rx_op_episode['total_rx_MME']/rx_op_episode['PAY_DAY_SUPPLY_CNT']\n",
    "rx_op_episode['consumed_MME_per_day'] = 1*rx_op_episode['total_rx_MME']/rx_op_episode['actual_dos']\n",
    "rx_op_episode['in_next_180_days'] = np.where((rx_op_episode['episode_start_day']>=0)&(rx_op_episode['episode_start_day']<180),1,0)\n",
    "rx_op_episode['cutoff_180_day'] = np.where((rx_op_episode['in_next_180_days']==1)&(rx_op_episode['episode_end_day']<180),rx_op_episode['episode_end_day'],180)\n",
    "\n",
    "rx_op_episode.reset_index(inplace=True)\n",
    "for index, row in rx_op_episode.iterrows():\n",
    "    if rx_op_episode.loc[index,'in_next_180_days']==1:\n",
    "        if (rx_op_episode.loc[index,'episode_end_day']<180):\n",
    "            rx_op_episode.loc[index,'cutoff_180_day'] = rx_op_episode.loc[index,'episode_end_day']-rx_op_episode.loc[index,'episode_start_day']\n",
    "        else:\n",
    "            rx_op_episode.loc[index,'cutoff_180_day'] = 180-rx_op_episode.loc[index,'episode_start_day']\n",
    "    else:\n",
    "        rx_op_episode.loc[index,'cutoff_180_day'] = 0\n",
    "        \n",
    "rx_op_episode_180 = rx_op_episode[rx_op_episode['in_next_180_days']==1]\n",
    "rx_op_episode_180_1 = rx_op_episode_180.groupby(['id'])['cutoff_180_day'].sum()\n",
    "rx_op_episode_180_1 = pd.DataFrame({'id':rx_op_episode_180_1.index, 'days_on_op_next_180':rx_op_episode_180_1.values})\n",
    "rx_op_episode_180_1['LTOT_flag'] = np.where(rx_op_episode_180_1['days_on_op_next_180']>=162,1,0)\n",
    "rx_op_episode =  rx_op_episode.merge(rx_op_episode_180_1,\n",
    "                                      left_on='id',\n",
    "                                      right_on='id',\n",
    "                                      how='left',\n",
    "                                      suffixes=['_ep', '_LTOT'])\n",
    "rx_op_episode['category'] = np.where(rx_op_episode['episode_start_day']<0,'pre','post')\n",
    "\n",
    "print (rx_op_episode.shape)\n",
    "\n",
    "rx_op_episode[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# holdout feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# ## coding parameters\n",
    "# import re\n",
    "warnings.filterwarnings('ignore')\n",
    "col = ['id','event_descr','event_attr1','event_attr2','event_attr3','event_attr4','event_attr5','Days_d']\n",
    "drop_col = ['event_attr1','event_attr2','event_attr3','event_attr4','event_attr5','event_attr6','event_attr7','event_attr8']\n",
    "\n",
    "# ################################\n",
    "# Feature extraction:\n",
    "# Pre/Post-\n",
    "# Total number of opioid episodes\n",
    "# total length of episode:sum(episode_len)\n",
    "# total length of therapy:max(episode_end)-min(episode_start)\n",
    "\n",
    "# Gap from last Opioid before initiation : \n",
    "    \n",
    "# ################################\n",
    "\n",
    "# data_ho=data_ho[:1000]\n",
    "data_dx = data_ho[(data_ho['event_category']=='Claim')]\n",
    "data_dx['pre_flag'] = np.where(data_dx['Days']<=0,1,0)\n",
    "\n",
    "# --------------Opioid intake related features\n",
    "aggregation = {'actual_dos':sum,'PAY_DAY_SUPPLY_CNT':sum,'PAYABLE_QTY':sum,'total_rx_MME':sum,\n",
    "              'rx_cost':sum,'mem_res_cost':sum}        \n",
    "id_op_mat = rx_op_episode.groupby(['id','category']).agg(aggregation)\n",
    "id_op_mat.reset_index(inplace=True)\n",
    "id_op_mat = id_op_mat[id_op_mat['category']=='pre']\n",
    "id_op_mat.drop('category',axis=1,inplace=True)\n",
    "id_op_mat.set_index('id',inplace=True)\n",
    "id_op_mat.shape\n",
    "\n",
    "# ------ Latest Dx before initiation (day=0) and days since last Dx\n",
    "data_dx_pre = data_dx[(data_dx['pre_flag']==1)&(data_dx['event_descr']=='Fully Paid Claim')]\n",
    "# data_dx_pre = data_dx_pre[data_dx_pre['id_d']== 'ID71291582272'] \n",
    "idx = data_dx_pre.groupby(['id'])['Days'].transform(max) == data_dx_pre['Days']\n",
    "latest_dx = data_dx_pre[idx]        \n",
    "latest_dx1 = latest_dx.groupby(['id','event_attr1','event_attr2','Days'])[['id']].count().rename(columns={'id':'count'})\n",
    "latest_dx1.reset_index(inplace=True)\n",
    "latest_dx1['rank_count'] = latest_dx1.groupby(['id'])['count'].rank(method='first',ascending=False)\n",
    "latest_dx_id = latest_dx1[latest_dx1['rank_count']==1]\n",
    "latest_dx_id = latest_dx_id[['id','event_attr1','event_attr2','Days']].rename(columns={'event_attr1':'latest_dx_desc','event_attr2':'latest_dx_pot','Days':'Days_last_dx'})\n",
    "latest_dx_id.set_index('id',inplace=True)\n",
    "\n",
    "# ------ other Dx features\n",
    "data_dx_pre['tot_charge_amt'] = data_dx_pre['event_attr3'].map(to_float)\n",
    "data_dx_pre['net_paid_amt'] = data_dx_pre['event_attr4'].map(to_float)\n",
    "data_dx_pre['mbr_res_amt'] = data_dx_pre['event_attr5'].map(to_float)\n",
    "data_dx_pre['mbr_res_amt'] = np.where(data_dx_pre['mbr_res_amt']<0,0,data_dx_pre['mbr_res_amt'])\n",
    "feature_dx1 = data_dx_pre.groupby(['id'])[['tot_charge_amt','net_paid_amt','mbr_res_amt']].sum()\n",
    "feature_dx1 = feature_dx1.reset_index()\n",
    "feature_dx1.set_index('id',inplace=True)\n",
    "\n",
    "# feature_dx1.plot.scatter(x='net_paid_amt',y='mbr_res_amt',c='LTOT_flag',colormap='viridis',sharex=False)\n",
    "# feature_dx1.hist(bins=25, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=1.5)\n",
    "# show()\n",
    "# feature_dx1.describe()\n",
    "\n",
    "# ------ latest New Dx before initiation (day=0) and days since last new Dx\n",
    "data_new_dx = data_ho[(data_ho['event_category']=='Diagnosis')]\n",
    "data_new_dx['pre_flag'] = np.where(data_new_dx['Days']<=0,1,0)\n",
    "data_new_dx_pre = data_new_dx[(data_new_dx['pre_flag']==1)]\n",
    "nidx = data_new_dx_pre.groupby(['id'])['Days'].transform(max) == data_new_dx_pre['Days']\n",
    "latest_new_dx = data_new_dx_pre[nidx]  \n",
    "latest_new_dx['new_dx_CPD'] = np.where(latest_new_dx['event_descr']=='New diagnosis - CPD',1,0)\n",
    "latest_new_dx['new_dx_hypertension'] = np.where(latest_new_dx['event_descr']=='New diagnosis - Hypertension',1,0)\n",
    "latest_new_dx['new_dx_top5'] = np.where(latest_new_dx['event_descr']=='New diagnosis - Top 5',1,0)\n",
    "latest_new_dx['new_dx_CAD'] = np.where(latest_new_dx['event_descr']=='New diagnosis - CAD',1,0)\n",
    "latest_new_dx['new_dx_diabetes'] = np.where(latest_new_dx['event_descr']=='New diagnosis - Diabetes',1,0)\n",
    "latest_new_dx['new_dx_CHF'] = np.where(latest_new_dx['event_descr']=='New diagnosis - CHF',1,0)\n",
    "latest_new_dx_id = latest_new_dx.groupby(['id','Days'])['new_dx_CPD','new_dx_hypertension','new_dx_top5','new_dx_CAD','new_dx_diabetes','new_dx_CHF'].max()\n",
    "latest_new_dx_id = latest_new_dx_id.reset_index().rename(columns={'Days':'latest_new_dx_day'})\n",
    "latest_new_dx_id.set_index('id',inplace=True)\n",
    "latest_new_dx_id.describe()\n",
    "# latest_new_dx_id[:5]\n",
    "# 5711 patients with new dx\n",
    "\n",
    "# ------- calls related features\n",
    "data_call = data_ho[(data_ho['event_category']=='Call')]\n",
    "data_call['pre_flag'] = np.where(data_call['Days']<=0,1,0)\n",
    "data_call_pre = data_call[data_call['pre_flag']==1]\n",
    "data_call_pre_id = data_call_pre.groupby(['id','event_descr'])[['id']].count().rename(columns={'id':'number_calls'})\n",
    "data_call_pre_id.reset_index(inplace=True)\n",
    "data_call_pre_id['call_by_mbr'] = np.where(data_call_pre_id['event_descr']=='Inbound Call by Mbr',data_call_pre_id['number_calls'],0)\n",
    "data_call_pre_id['call_by_prov'] = np.where(data_call_pre_id['event_descr']=='Inbound Call by Prov',data_call_pre_id['number_calls'],0)\n",
    "data_call_pre_id['call_by_other'] = np.where(data_call_pre_id['event_descr']=='Inbound Call by Other',data_call_pre_id['number_calls'],0)\n",
    "feature_data_call = data_call_pre_id.groupby(['id'])['call_by_mbr','call_by_prov','call_by_other'].max()\n",
    "feature_data_call = feature_data_call.reset_index()\n",
    "feature_data_call.set_index('id',inplace=True)\n",
    "# feature_data_call[:5]\n",
    "\n",
    "# ------- provider related features\n",
    "data_prov = data_ho[(data_ho['event_category']=='Provider')]\n",
    "data_prov['pre_flag'] = np.where(data_prov['Days']<=0,1,0)\n",
    "data_prov_pre = data_prov[data_prov['pre_flag']==1]\n",
    "pidx = data_prov_pre.groupby(['id'])['Days'].transform(max) == data_prov_pre['Days']\n",
    "latest_new_prov = data_prov_pre[pidx]  \n",
    "latest_new_prov = latest_new_prov[['id','Days']].rename(columns={'Days':'latest_new_prov_date'})\n",
    "# latest_new_prov = latest_new_prov.reset_index()\n",
    "latest_new_prov.set_index('id',inplace=True)\n",
    "\n",
    "# ------ Latest Surgery before initiation (day=0) and days since last surgery\n",
    "data_sx_pre = data_dx[(data_dx['pre_flag']==1)&(data_dx['event_descr']=='Surgery')]\n",
    "# data_sx_pre = data_sx_pre[data_sx_pre['id_d']== 'ID99421949451'] \n",
    "idsx = data_sx_pre.groupby(['id'])['Days'].transform(max) == data_sx_pre['Days']\n",
    "latest_sx = data_sx_pre[idsx]\n",
    "\n",
    "latest_sx1 = latest_sx.groupby(['id','event_attr1','event_attr2','Days'])[['id']].count().rename(columns={'id':'count'})\n",
    "latest_sx1.reset_index(inplace=True)\n",
    "latest_sx1['rank_count'] = latest_sx1.groupby(['id'])['count'].rank(method='first',ascending=False)\n",
    "latest_sx_id = latest_sx1[latest_sx1['rank_count']==1]\n",
    "latest_sx_id = latest_sx_id[['id','event_attr1','event_attr2','Days']].rename(columns={'event_attr1':'latest_sx_desc','event_attr2':'latest_sx_pot','Days':'Days_last_sx'})\n",
    "latest_sx_id.set_index('id',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------Rx feature code\n",
    "rx_other_paid = rx[rx['event_descr'] == 'RX Claim - Paid']\n",
    "\n",
    "rx_other_paid['tx_cat'] = rx_other_paid['event_attr6'].map(tx_cat)\n",
    "rx_other_paid['tx_cat'] = np.where(~rx_other_paid['MME'].isnull(),'Opioid Treatment',rx_other_paid['tx_cat'])\n",
    "\n",
    "#taking only negative day claims for analysis further\n",
    "rx_other_paid = rx_other_paid[rx_other_paid['Days'] <0]\n",
    "\n",
    "# Total claim count\n",
    "rx_other_paid0 = pd.DataFrame(rx_other_paid.groupby('id')['id'].agg('count'))\n",
    "rx_other_paid0.columns = ['claim_count']\n",
    "rx_other_paid0.reset_index(inplace=True)\n",
    "rx_other_paid0.set_index('id',inplace=True)\n",
    "\n",
    "# Total claim count by category\n",
    "rx_other_paid1 = pd.DataFrame(rx_other_paid.groupby(['id', 'tx_cat'])['id'].agg('count'))\n",
    "rx_other_paid1.columns = ['claim_count']\n",
    "rx_other_paid1.reset_index(inplace=True)\n",
    "rx_other_paid2 = pd.DataFrame(pd.pivot_table(rx_other_paid1, index = 'id', columns = 'tx_cat', values = 'claim_count')).reset_index()\n",
    "rx_other_paid2.fillna(0, inplace=True)\n",
    "rx_other_paid2.set_index('id',inplace=True)\n",
    "\n",
    "# Average gap between two claims of same category\n",
    "rx_other_paid['claim_end_day'] = rx_other_paid['Days']+rx_other_paid['PAY_DAY_SUPPLY_CNT']\n",
    "rx_other_paid3 = rx_other_paid[['id', 'tx_cat', 'Days', 'claim_end_day']]\n",
    "rx_other_paid3['same_cat_next_claim_day'] = rx_other_paid3.groupby([\"id\",\"tx_cat\"])['Days'].shift(-1)\n",
    "rx_other_paid3['next_claim_day'] = rx_other_paid3.groupby(\"id\")['Days'].shift(-1)\n",
    "rx_other_paid3['same_cat_gap'] = rx_other_paid3['same_cat_next_claim_day']-rx_other_paid3['Days']\n",
    "rx_other_paid3['gap'] = rx_other_paid3['next_claim_day']-rx_other_paid3['Days']\n",
    "rx_other_paid4 = pd.DataFrame(rx_other_paid3.groupby(['id','tx_cat'])['same_cat_gap'].mean())\n",
    "rx_other_paid4.columns = ['same_cat_avg_gap']\n",
    "rx_other_paid4.reset_index(inplace=True)\n",
    "rx_other_paid4 = pd.DataFrame(pd.pivot_table(rx_other_paid4, index = 'id', columns = 'tx_cat', values = 'same_cat_avg_gap')).reset_index()\n",
    "rx_other_paid4.fillna(0, inplace=True)\n",
    "rx_other_paid4.set_index('id',inplace=True)\n",
    "\n",
    "# Average gap between two claims overall\n",
    "rx_other_paid5 = pd.DataFrame(rx_other_paid3.groupby('id')['gap'].mean())\n",
    "rx_other_paid5.columns = ['avg_gap']\n",
    "rx_other_paid5.reset_index(inplace=True)\n",
    "rx_other_paid5.set_index('id',inplace=True)\n",
    "\n",
    "# Last claim before Day 0\n",
    "rx_other_paid['gap_from_last_claim_end'] = np.where((rx_other_paid['PAY_DAY_SUPPLY_CNT'].isnull()),0-rx_other_paid['Days'],0-(rx_other_paid['Days']+rx_other_paid['PAY_DAY_SUPPLY_CNT']))\n",
    "rx_other_paid6 = rx_other_paid.copy(deep=True)\n",
    "rx_other_paid6['id_claim_rank'] = rx_other_paid6.groupby('id')['Days'].rank(method='first', ascending=False)\n",
    "rx_other_paid6 = rx_other_paid6[rx_other_paid6['id_claim_rank'] == 1]\n",
    "rx_other_paid6.set_index('id',inplace=True)\n",
    "\n",
    "# Last Opioid claim before Day 0\n",
    "mask2 = rx_other_paid['MME'].notnull()\n",
    "rx_other_paid7 = rx_other_paid.loc[mask2]\n",
    "rx_other_paid7['opioid_claim_rank'] = rx_other_paid7.groupby('id')['Days'].rank(method='first', ascending=False)\n",
    "rx_other_paid7 = rx_other_paid7[rx_other_paid7['opioid_claim_rank'] == 1]\n",
    "rx_other_paid7.set_index('id',inplace=True)\n",
    "\n",
    "# Sum of all amounts\n",
    "rx_other_paid8 = rx_other_paid.copy(deep=True)\n",
    "rx_other_paid8['event_attr3'] = rx_other_paid8['event_attr3'].map(to_float)\n",
    "rx_other_paid8['event_attr4'] = rx_other_paid8['event_attr4'].map(to_float)\n",
    "rx_other_paid8.fillna(0, inplace=True)\n",
    "rx_other_paid9 = rx_other_paid8.groupby(['id'])[['event_attr3', 'event_attr4', 'event_attr9']].sum()\n",
    "rx_other_paid9.columns = ['tot_rx_cost', 'tot_net_paid_amt', 'tot_mem_res_amt']\n",
    "rx_other_paid9.reset_index(inplace=True)\n",
    "rx_other_paid9.set_index('id',inplace=True)\n",
    "\n",
    "\n",
    "# Rejected claims\n",
    "rx_rejected = rx[(rx['event_descr'] == 'RX Claim - Rejected')&(rx['Days']<0)]\n",
    "\n",
    "# Total rejected claim count\n",
    "rx_rejected1 = pd.DataFrame(rx_rejected.groupby('id')['id'].count())\n",
    "rx_rejected1.columns = ['rejected_claim_count']\n",
    "rx_rejected1.reset_index(inplace=True)\n",
    "rx_rejected1.set_index('id',inplace=True)\n",
    "\n",
    "\n",
    "# Total rejected claim count by reason\n",
    "rx_rejected2 = pd.DataFrame(rx_rejected.groupby(['id', 'event_attr1'])['id'].count())\n",
    "rx_rejected2.columns = ['reason_rejected_claim_count']\n",
    "rx_rejected2.reset_index(inplace = True)\n",
    "rx_rejected3 = pd.DataFrame(pd.pivot_table(rx_rejected2, index = 'id', columns = 'event_attr1', \n",
    "                                           values = 'reason_rejected_claim_count')).reset_index()\n",
    "rx_rejected3.fillna(0, inplace=True)\n",
    "\n",
    "# Total first mail order claim count\n",
    "rx_first_mail = rx[rx['event_descr'] == 'RX Claim - First Time Mail Order']\n",
    "rx_first_mail1 = pd.DataFrame(rx_first_mail.groupby('id')['id'].count())\n",
    "rx_first_mail1.columns = ['first_mail_claim_count']\n",
    "rx_first_mail1.reset_index(inplace=True)\n",
    "rx_first_mail1.set_index('id',inplace=True)\n",
    "\n",
    "# ------List of all unique id\n",
    "all_id = data_ho.groupby('id')['id'].count().rename(columns={'id':'count'})\n",
    "all_id = pd.DataFrame({'id':all_id.index, 'no_records':all_id.values})\n",
    "print(len(all_id))\n",
    "\n",
    "# rx_first_mail1[:10]\n",
    "# rx_other_paid2\n",
    "# rx_other_paid5[:2]\n",
    "# rx_other_paid7[:10]\n",
    "\n",
    "\n",
    "print ('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column-wise missing value percentage\n",
      "id                           0.0\n",
      "no_records                   0.0\n",
      "latest_dx_desc               0.0\n",
      "latest_dx_pot                0.0\n",
      "Days_last_dx                 0.0\n",
      "latest_new_dx_day            0.0\n",
      "new_dx_CPD                   0.0\n",
      "new_dx_hypertension          0.0\n",
      "new_dx_top5                  0.0\n",
      "new_dx_CAD                   0.0\n",
      "new_dx_diabetes              0.0\n",
      "new_dx_CHF                   0.0\n",
      "dx_tot_charge_amt            0.0\n",
      "dx_net_paid_amt              0.0\n",
      "dx_mbr_res_amt               0.0\n",
      "latest_new_prov_date         0.0\n",
      "call_by_mbr                  0.0\n",
      "call_by_prov                 0.0\n",
      "call_by_other                0.0\n",
      "actual_dos_op_pre            0.0\n",
      "dos_op_pre                   0.0\n",
      "op_qty_pre                   0.0\n",
      "MME                          0.0\n",
      "tot_rx_cost_op               0.0\n",
      "mem_res_cost_op              0.0\n",
      "latest_sx_desc               0.0\n",
      "latest_sx_pot                0.0\n",
      "Days_last_sx                 0.0\n",
      "last_op_claim_end_day        0.0\n",
      "claims_op_pre                0.0\n",
      "claims_oth_pre               0.0\n",
      "claims_pain_pre              0.0\n",
      "rejected_claim_count         0.0\n",
      "tot_rx_cost                  0.0\n",
      "tot_net_paid_amt             0.0\n",
      "tot_mem_res_amt              0.0\n",
      "no_new_dx_flag               0.0\n",
      "no_new_sx_flag               0.0\n",
      "on_op_pre                    0.0\n",
      "Days_last_sx_bkt             0.0\n",
      "latest_new_dx_day_bkt        0.0\n",
      "last_op_claim_end_day_bkt    0.0\n",
      "actual_dos_op_pre_bkt        0.0\n",
      "rejection_ratio              0.0\n",
      "net_mbr_res_amt              0.0\n",
      "op_rx_cost_share             0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# ------- Feature matrix creation\n",
    "feature_mat0 = all_id.join(latest_dx_id, on='id', how='left')\n",
    "feature_mat1 = feature_mat0.join(latest_new_dx_id, on='id',how='left')\n",
    "feature_mat2 = feature_mat1.join(feature_dx1, on='id',how='left')\n",
    "feature_mat3 = feature_mat2.join(latest_new_prov, on='id',how='left')\n",
    "feature_mat4 = feature_mat3.join(feature_data_call, on='id',how='left')\n",
    "feature_mat5 = feature_mat4.join(id_op_mat, on='id',how='left')\n",
    "feature_mat6 = feature_mat5.join(latest_sx_id, on='id',how='left')\n",
    "feature_mat7 = feature_mat6.join(rx_other_paid7['claim_end_day'], on='id',how='left')\n",
    "feature_mat8 = feature_mat7.join(rx_other_paid2, on='id',how='left')\n",
    "feature_mat9 = feature_mat8.join(rx_rejected1, on='id',how='left')\n",
    "feature_mat = feature_mat9.join(rx_other_paid9, on='id',how='left')\n",
    "\n",
    "feature_mat['no_new_dx_flag'] = np.where(feature_mat['latest_new_dx_day'].isnull(),1,0) \n",
    "feature_mat['no_new_sx_flag'] = np.where(feature_mat['Days_last_sx'].isnull(),1,0)\n",
    "feature_mat['on_op_pre'] = np.where(~feature_mat['actual_dos'].isnull(),1,0)\n",
    "feature_mat['Days_last_sx_bkt'] = pd.qcut(feature_mat['Days_last_sx'].values, 5).codes\n",
    "feature_mat['latest_new_dx_day_bkt'] = pd.qcut(feature_mat['latest_new_dx_day'].values, 5).codes\n",
    "feature_mat['claim_end_day_bkt'] = pd.qcut(feature_mat['claim_end_day'].values, 5).codes\n",
    "feature_mat['actual_dos_bkt'] = pd.qcut(feature_mat['actual_dos'].values, 5).codes\n",
    "feature_mat['rejection_ratio'] = 1*(feature_mat['rejected_claim_count'])/(feature_mat['Opioid Treatment']+feature_mat['Other Treatment']+feature_mat['Pain Treatment'])\n",
    "# feature_mat['Days_last_dx_bkt'] = pd.qcut(feature_mat['Days_last_dx'].values, 5).codes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bins = np.array([0,5,25,50,150,250,1000,5000,10000])\n",
    "values = {'new_dx_CPD':0,'new_dx_hypertension': 0,'new_dx_top5': 0,'new_dx_CAD': 0,'new_dx_diabetes':0,\n",
    "          'new_dx_CHF':0,'latest_new_dx_day':0,'latest_sx_desc':'unk','latest_sx_pot':'unk',\n",
    "          'Days_last_sx':0,'latest_new_prov_date':0,'call_by_mbr':0,'call_by_other':0,'call_by_prov':0,\n",
    "         'actual_dos':0,'PAY_DAY_SUPPLY_CNT':0,'PAYABLE_QTY':0,'total_rx_MME':0,'rx_cost':0,'mem_res_cost':0,\n",
    "         'claim_end_day':0,'Opioid Treatment':0,'Other Treatment':0,'Pain Treatment':0,'rejected_claim_count':0,\n",
    "         'rejection_ratio':0,'latest_dx_desc':'unk','latest_dx_pot':'unk','Days_last_dx':0,\n",
    "         'tot_charge_amt':0,'net_paid_amt':0,'mbr_res_amt':0,'tot_rx_cost':0,\n",
    "         'tot_net_paid_amt':0,'tot_mem_res_amt':0,'op_rx_cost_share':0}\n",
    "\n",
    "feature_mat = feature_mat.fillna(value=values)\n",
    "feature_mat['net_mbr_res_amt'] = feature_mat['mbr_res_amt']+feature_mat['mem_res_cost']+feature_mat['tot_mem_res_amt']\n",
    "feature_mat['op_rx_cost_share'] = feature_mat['mem_res_cost']/feature_mat['tot_mem_res_amt']\n",
    "\n",
    "feature_mat_ho = feature_mat.fillna(value=values)\n",
    "\n",
    "feature_mat_ho.rename(columns={'actual_dos':'actual_dos_op_pre','PAY_DAY_SUPPLY_CNT':'dos_op_pre',\n",
    "                           'PAYABLE_QTY':'op_qty_pre','total_rx_MME':'MME','rx_cost':'tot_rx_cost_op',\n",
    "                           'mem_res_cost':'mem_res_cost_op','claim_end_day':'last_op_claim_end_day',\n",
    "                           'Opioid Treatment':'claims_op_pre','Other Treatment':'claims_oth_pre',\n",
    "                           'Pain Treatment':'claims_pain_pre','actual_dos_bkt':'actual_dos_op_pre_bkt',\n",
    "                           'tot_charge_amt':'dx_tot_charge_amt','net_paid_amt':'dx_net_paid_amt',\n",
    "                           'mbr_res_amt':'dx_mbr_res_amt','claim_end_day_bkt':'last_op_claim_end_day_bkt'},inplace=True)\n",
    "\n",
    "print(\"Column-wise missing value percentage\")\n",
    "print(feature_mat_ho.isnull().sum()/len(feature_mat_ho)*100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# feature_mat.columns#[:10]\n",
    "\n",
    "\n",
    "# feature_mat.describe()\n",
    "# feature_mat['LTOT_flag'].sum()/len(feature_mat)*100\n",
    "\n",
    "\n",
    "# feature_mat.plot.scatter(x='dx_mbr_res_amt',y='dx_tot_charge_amt',c='LTOT_flag',colormap='viridis',sharex=False)\n",
    "# xlabel('mbr_res_amt')\n",
    "# ylabel('tot_charge_amt')\n",
    "# show()\n",
    "\n",
    "# # mbr_res_amt\n",
    "# sc_var = ['claims_op_pre','claims_oth_pre','claims_pain_pre']\n",
    "# # sc_var = ['actual_dos_op_pre','dos_op_pre','op_qty_pre','MME','tot_rx_cost_op','mem_res_cost_op','mbr_res_amt','last_op_claim_end_day']\n",
    "\n",
    "# corr = feature_mat[sc_var].corr()\n",
    "# ax = sns.heatmap(\n",
    "#     corr, \n",
    "#     vmin=-1, vmax=1, center=0,\n",
    "#     cmap=sns.diverging_palette(20, 220, n=200),\n",
    "#     square=True\n",
    "# )\n",
    "# ax.set_xticklabels(\n",
    "#     ax.get_xticklabels(),\n",
    "#     rotation=90,\n",
    "#     horizontalalignment='right'\n",
    "# #     ax.set_xticks(np.arange(15)\n",
    "# );\n",
    "# show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis/Surgery events ------ train processing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are patients with more than 1 day=0 events\n",
      "No of opioid claims taken on day0 and count of patient\n",
      "1    13445\n",
      "2      310\n",
      "0      240\n",
      "3        5\n",
      "Name: sum, dtype: int64\n",
      "(121547, 17)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------LTOT flag creation \n",
    "\n",
    "# ##Patients with >1 initiation (days=0)\n",
    "rx = data_train[(data_train['event_category']=='RxClaim')]\n",
    "rx.shape\n",
    "rx_paid_claim = rx[(rx['event_descr']=='RX Claim - Paid') & (rx['Days']==0)]\n",
    "rx_paid_claim_pat = rx_paid_claim.groupby(['id'])[['event_descr']].count().rename(columns={'event_descr':'count'})\n",
    "rx_paid_claim_pat['count'].value_counts().plot(kind='bar')\n",
    "xlabel('#Initiation claims')\n",
    "ylabel('#Patients')\n",
    "show()\n",
    "print ('There are patients with more than 1 day=0 events') # ##ID10081072715 :3 claims on day 0\n",
    "\n",
    "# ## Naive Opioid claims\n",
    "rx['Naive_opioid_claim'] = np.where((rx['event_descr']=='RX Claim - Paid')& (rx['Days']==0) & (~rx['MME'].isnull()),1,0)\n",
    "rx_naive_claims_pat = rx.groupby(['id'])[['Naive_opioid_claim']].sum().rename(columns={'Naive_opioid_claim':'sum'})\n",
    "print ('No of opioid claims taken on day0 and count of patient')\n",
    "print (rx_naive_claims_pat['sum'].value_counts())\n",
    "\n",
    "\n",
    "# ##Opioid Episode level summary with LTOT flag\n",
    "rx_op_claim = rx[(rx['event_descr']=='RX Claim - Paid')&(~rx['MME'].isnull())].sort_values(['id','Days'])\n",
    "rx_op_claim['total_rx_MME'] = rx_op_claim['MME']*rx_op_claim['PAYABLE_QTY']\n",
    "rx_op_claim['rx_cost'] = rx_op_claim['event_attr3'].map(to_float)\n",
    "rx_op_claim['mem_res_cost'] = rx_op_claim['event_attr9'].map(to_float)\n",
    "rx_op_claim = rx_op_claim.groupby(['id','Days'])[['rx_cost','mem_res_cost','PAY_DAY_SUPPLY_CNT','PAYABLE_QTY','total_rx_MME']].sum()\n",
    "rx_op_claim.reset_index(inplace=True)\n",
    "rx_op_claim['MME'] = 1*rx_op_claim['total_rx_MME']/rx_op_claim['PAY_DAY_SUPPLY_CNT']\n",
    "\n",
    "grace = 0\n",
    "# rx_op_claim = rx_op_claim[rx_op_claim['id']=='ID10010854159']\n",
    "# ID10010854159\n",
    "# ID10433575841\n",
    "rx_op_claim['claim_end_day'] = rx_op_claim['Days']+rx_op_claim['PAY_DAY_SUPPLY_CNT']\n",
    "rx_op_claim['next_op'] = rx_op_claim.groupby(\"id\")['Days'].shift(-1) \n",
    "rx_op_claim['actual_dos'] = np.where((rx_op_claim['claim_end_day']<rx_op_claim['next_op'])|(rx_op_claim['next_op'].isnull()),rx_op_claim['PAY_DAY_SUPPLY_CNT'],rx_op_claim['next_op']-rx_op_claim['Days'])\n",
    "rx_op_claim['gap_to_next_op'] = rx_op_claim['next_op']-rx_op_claim['claim_end_day']\n",
    "rx_op_claim['previous_op_end_day'] = rx_op_claim.groupby(\"id\")['claim_end_day'].shift(1) \n",
    "rx_op_claim['gap_from_previous_op'] = rx_op_claim['Days']-rx_op_claim['previous_op_end_day']\n",
    "rx_op_claim['new_patient_flag'] = np.where((rx_op_claim['gap_from_previous_op'].isnull())|(rx_op_claim['gap_from_previous_op']>90),1,0)\n",
    "\n",
    "# rx_op_claim.groupby('id')['new_patient_flag'].sum().value_counts().plot(kind='bar')\n",
    "\n",
    "rx_op_claim['is_episode_start'] = np.where((rx_op_claim['gap_from_previous_op']>grace)|(rx_op_claim['gap_from_previous_op'].isnull()),1,0)\n",
    "rx_op_claim['is_episode_end'] = np.where((rx_op_claim['next_op']>rx_op_claim['claim_end_day']+grace)|(rx_op_claim['gap_to_next_op'].isnull()),1,0)\n",
    "\n",
    "rx_op_claim['episode_start_day'] = np.where((rx_op_claim['gap_from_previous_op']>grace)|(rx_op_claim['gap_from_previous_op'].isnull()),rx_op_claim['Days'],999999999)\n",
    "rx_op_claim['episode_end_day'] = np.where((rx_op_claim['next_op']>rx_op_claim['claim_end_day']+grace)|(rx_op_claim['gap_to_next_op'].isnull()),rx_op_claim['claim_end_day'],-99999999)\n",
    "rx_op_claim['episode_rank'] = rx_op_claim.groupby(['id','is_episode_start'])['episode_start_day'].rank(method='dense')\n",
    "\n",
    "\n",
    "rx_op_claim.reset_index(inplace=True)\n",
    "for index, row in rx_op_claim.iterrows():      \n",
    "    if int(index)==0:\n",
    "        rx_op_claim.loc[index,'episode_num'] = 1\n",
    "    else:\n",
    "        if (rx_op_claim.loc[index,'is_episode_start']==0)&(rx_op_claim.loc[index,'episode_rank']==1):\n",
    "            rx_op_claim.loc[index,'episode_num'] = rx_op_claim.loc[int(index)-1,'episode_num']\n",
    "        else:\n",
    "            rx_op_claim.loc[index,'episode_num'] = rx_op_claim.loc[index,'episode_rank']\n",
    "\n",
    "aggregation = {'episode_start_day':min,'episode_end_day':max,'actual_dos':sum,\n",
    "               'PAY_DAY_SUPPLY_CNT':sum,'PAYABLE_QTY':sum,'total_rx_MME':sum,\n",
    "               'rx_cost':sum,'mem_res_cost':sum}        \n",
    "rx_op_episode = rx_op_claim.groupby(['id','episode_num']).agg(aggregation)\n",
    "rx_op_episode['prescribed_MME_per_day'] = 1*rx_op_episode['total_rx_MME']/rx_op_episode['PAY_DAY_SUPPLY_CNT']\n",
    "rx_op_episode['consumed_MME_per_day'] = 1*rx_op_episode['total_rx_MME']/rx_op_episode['actual_dos']\n",
    "rx_op_episode['in_next_180_days'] = np.where((rx_op_episode['episode_start_day']>=0)&(rx_op_episode['episode_start_day']<180),1,0)\n",
    "rx_op_episode['cutoff_180_day'] = np.where((rx_op_episode['in_next_180_days']==1)&(rx_op_episode['episode_end_day']<180),rx_op_episode['episode_end_day'],180)\n",
    "\n",
    "rx_op_episode.reset_index(inplace=True)\n",
    "for index, row in rx_op_episode.iterrows():\n",
    "    if rx_op_episode.loc[index,'in_next_180_days']==1:\n",
    "        if (rx_op_episode.loc[index,'episode_end_day']<180):\n",
    "            rx_op_episode.loc[index,'cutoff_180_day'] = rx_op_episode.loc[index,'episode_end_day']-rx_op_episode.loc[index,'episode_start_day']\n",
    "        else:\n",
    "            rx_op_episode.loc[index,'cutoff_180_day'] = 180-rx_op_episode.loc[index,'episode_start_day']\n",
    "    else:\n",
    "        rx_op_episode.loc[index,'cutoff_180_day'] = 0\n",
    "        \n",
    "rx_op_episode_180 = rx_op_episode[rx_op_episode['in_next_180_days']==1]\n",
    "rx_op_episode_180_1 = rx_op_episode_180.groupby(['id'])['cutoff_180_day'].sum()\n",
    "rx_op_episode_180_1 = pd.DataFrame({'id':rx_op_episode_180_1.index, 'days_on_op_next_180':rx_op_episode_180_1.values})\n",
    "rx_op_episode_180_1['LTOT_flag'] = np.where(rx_op_episode_180_1['days_on_op_next_180']>=162,1,0)\n",
    "rx_op_episode =  rx_op_episode.merge(rx_op_episode_180_1,\n",
    "                                      left_on='id',\n",
    "                                      right_on='id',\n",
    "                                      how='left',\n",
    "                                      suffixes=['_ep', '_LTOT'])\n",
    "rx_op_episode['category'] = np.where(rx_op_episode['episode_start_day']<0,'pre','post')\n",
    "\n",
    "ltot = rx_op_episode.groupby(['id'])['LTOT_flag'].max()\n",
    "ltot = pd.DataFrame({'id':ltot.index, 'LTOT_flag':ltot.values})\n",
    "\n",
    "print (rx_op_episode.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# ## coding parameters\n",
    "# import re\n",
    "warnings.filterwarnings('ignore')\n",
    "col = ['id','event_descr','event_attr1','event_attr2','event_attr3','event_attr4','event_attr5','Days_d']\n",
    "drop_col = ['event_attr1','event_attr2','event_attr3','event_attr4','event_attr5','event_attr6','event_attr7','event_attr8']\n",
    "\n",
    "# ################################\n",
    "# Feature extraction:\n",
    "# Pre/Post-\n",
    "# Total number of opioid episodes\n",
    "# total length of episode:sum(episode_len)\n",
    "# total length of therapy:max(episode_end)-min(episode_start)\n",
    "\n",
    "# Gap from last Opioid before initiation : \n",
    "    \n",
    "# ################################\n",
    "\n",
    "data_dx = data_train[(data_train['event_category']=='Claim')]\n",
    "data_dx['pre_flag'] = np.where(data_dx['Days']<=0,1,0)\n",
    "\n",
    "# --------------Opioid intake related features\n",
    "aggregation = {'actual_dos':sum,'PAY_DAY_SUPPLY_CNT':sum,'PAYABLE_QTY':sum,'total_rx_MME':sum,\n",
    "              'rx_cost':sum,'mem_res_cost':sum}        \n",
    "id_op_mat = rx_op_episode.groupby(['id','category']).agg(aggregation)\n",
    "id_op_mat.reset_index(inplace=True)\n",
    "id_op_mat = id_op_mat[id_op_mat['category']=='pre']\n",
    "id_op_mat.drop('category',axis=1,inplace=True)\n",
    "id_op_mat.set_index('id',inplace=True)\n",
    "id_op_mat.shape\n",
    "\n",
    "# ------ Latest Dx before initiation (day=0) and days since last Dx\n",
    "data_dx_pre = data_dx[(data_dx['pre_flag']==1)&(data_dx['event_descr']=='Fully Paid Claim')]\n",
    "# data_dx_pre = data_dx_pre[data_dx_pre['id_d']== 'ID71291582272'] \n",
    "idx = data_dx_pre.groupby(['id'])['Days'].transform(max) == data_dx_pre['Days']\n",
    "latest_dx = data_dx_pre[idx]        \n",
    "latest_dx1 = latest_dx.groupby(['id','event_attr1','event_attr2','Days'])[['id']].count().rename(columns={'id':'count'})\n",
    "latest_dx1.reset_index(inplace=True)\n",
    "latest_dx1['rank_count'] = latest_dx1.groupby(['id'])['count'].rank(method='first',ascending=False)\n",
    "latest_dx_id = latest_dx1[latest_dx1['rank_count']==1]\n",
    "latest_dx_id = latest_dx_id[['id','event_attr1','event_attr2','Days']].rename(columns={'event_attr1':'latest_dx_desc','event_attr2':'latest_dx_pot','Days':'Days_last_dx'})\n",
    "latest_dx_id.set_index('id',inplace=True)\n",
    "\n",
    "# ------ other Dx features\n",
    "data_dx_pre['tot_charge_amt'] = data_dx_pre['event_attr3'].map(to_float)\n",
    "data_dx_pre['net_paid_amt'] = data_dx_pre['event_attr4'].map(to_float)\n",
    "data_dx_pre['mbr_res_amt'] = data_dx_pre['event_attr5'].map(to_float)\n",
    "data_dx_pre['mbr_res_amt'] = np.where(data_dx_pre['mbr_res_amt']<0,0,data_dx_pre['mbr_res_amt'])\n",
    "feature_dx1 = data_dx_pre.groupby(['id'])[['tot_charge_amt','net_paid_amt','mbr_res_amt']].sum()\n",
    "feature_dx1 = feature_dx1.reset_index()\n",
    "feature_dx1.set_index('id',inplace=True)\n",
    "\n",
    "# feature_dx1.plot.scatter(x='net_paid_amt',y='mbr_res_amt',c='LTOT_flag',colormap='viridis',sharex=False)\n",
    "# feature_dx1.hist(bins=25, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=1.5)\n",
    "# show()\n",
    "# feature_dx1.describe()\n",
    "\n",
    "# ------ latest New Dx before initiation (day=0) and days since last new Dx\n",
    "data_new_dx = data_train[(data_train['event_category']=='Diagnosis')]\n",
    "data_new_dx['pre_flag'] = np.where(data_new_dx['Days']<=0,1,0)\n",
    "data_new_dx_pre = data_new_dx[(data_new_dx['pre_flag']==1)]\n",
    "nidx = data_new_dx_pre.groupby(['id'])['Days'].transform(max) == data_new_dx_pre['Days']\n",
    "latest_new_dx = data_new_dx_pre[nidx]  \n",
    "latest_new_dx['new_dx_CPD'] = np.where(latest_new_dx['event_descr']=='New diagnosis - CPD',1,0)\n",
    "latest_new_dx['new_dx_hypertension'] = np.where(latest_new_dx['event_descr']=='New diagnosis - Hypertension',1,0)\n",
    "latest_new_dx['new_dx_top5'] = np.where(latest_new_dx['event_descr']=='New diagnosis - Top 5',1,0)\n",
    "latest_new_dx['new_dx_CAD'] = np.where(latest_new_dx['event_descr']=='New diagnosis - CAD',1,0)\n",
    "latest_new_dx['new_dx_diabetes'] = np.where(latest_new_dx['event_descr']=='New diagnosis - Diabetes',1,0)\n",
    "latest_new_dx['new_dx_CHF'] = np.where(latest_new_dx['event_descr']=='New diagnosis - CHF',1,0)\n",
    "latest_new_dx_id = latest_new_dx.groupby(['id','Days'])['new_dx_CPD','new_dx_hypertension','new_dx_top5','new_dx_CAD','new_dx_diabetes','new_dx_CHF'].max()\n",
    "latest_new_dx_id = latest_new_dx_id.reset_index().rename(columns={'Days':'latest_new_dx_day'})\n",
    "latest_new_dx_id.set_index('id',inplace=True)\n",
    "latest_new_dx_id.describe()\n",
    "# latest_new_dx_id[:5]\n",
    "# 5711 patients with new dx\n",
    "\n",
    "# ------- calls related features\n",
    "data_call = data_train[(data_train['event_category']=='Call')]\n",
    "data_call['pre_flag'] = np.where(data_call['Days']<=0,1,0)\n",
    "data_call_pre = data_call[data_call['pre_flag']==1]\n",
    "data_call_pre_id = data_call_pre.groupby(['id','event_descr'])[['id']].count().rename(columns={'id':'number_calls'})\n",
    "data_call_pre_id.reset_index(inplace=True)\n",
    "data_call_pre_id['call_by_mbr'] = np.where(data_call_pre_id['event_descr']=='Inbound Call by Mbr',data_call_pre_id['number_calls'],0)\n",
    "data_call_pre_id['call_by_prov'] = np.where(data_call_pre_id['event_descr']=='Inbound Call by Prov',data_call_pre_id['number_calls'],0)\n",
    "data_call_pre_id['call_by_other'] = np.where(data_call_pre_id['event_descr']=='Inbound Call by Other',data_call_pre_id['number_calls'],0)\n",
    "feature_data_call = data_call_pre_id.groupby(['id'])['call_by_mbr','call_by_prov','call_by_other'].max()\n",
    "feature_data_call = feature_data_call.reset_index()\n",
    "feature_data_call.set_index('id',inplace=True)\n",
    "# feature_data_call[:5]\n",
    "\n",
    "# ------- provider related features\n",
    "data_prov = data_train[(data_train['event_category']=='Provider')]\n",
    "data_prov['pre_flag'] = np.where(data_prov['Days']<=0,1,0)\n",
    "data_prov_pre = data_prov[data_prov['pre_flag']==1]\n",
    "pidx = data_prov_pre.groupby(['id'])['Days'].transform(max) == data_prov_pre['Days']\n",
    "latest_new_prov = data_prov_pre[pidx]  \n",
    "latest_new_prov = latest_new_prov[['id','Days']].rename(columns={'Days':'latest_new_prov_date'})\n",
    "# latest_new_prov = latest_new_prov.reset_index()\n",
    "latest_new_prov.set_index('id',inplace=True)\n",
    "\n",
    "# ------ Latest Surgery before initiation (day=0) and days since last surgery\n",
    "data_sx_pre = data_dx[(data_dx['pre_flag']==1)&(data_dx['event_descr']=='Surgery')]\n",
    "# data_sx_pre = data_sx_pre[data_sx_pre['id_d']== 'ID99421949451'] \n",
    "idsx = data_sx_pre.groupby(['id'])['Days'].transform(max) == data_sx_pre['Days']\n",
    "latest_sx = data_sx_pre[idsx]\n",
    "\n",
    "latest_sx1 = latest_sx.groupby(['id','event_attr1','event_attr2','Days'])[['id']].count().rename(columns={'id':'count'})\n",
    "latest_sx1.reset_index(inplace=True)\n",
    "latest_sx1['rank_count'] = latest_sx1.groupby(['id'])['count'].rank(method='first',ascending=False)\n",
    "latest_sx_id = latest_sx1[latest_sx1['rank_count']==1]\n",
    "latest_sx_id = latest_sx_id[['id','event_attr1','event_attr2','Days']].rename(columns={'event_attr1':'latest_sx_desc','event_attr2':'latest_sx_pot','Days':'Days_last_sx'})\n",
    "latest_sx_id.set_index('id',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------Rx feature code\n",
    "rx_other_paid = rx[rx['event_descr'] == 'RX Claim - Paid']\n",
    "\n",
    "rx_other_paid['tx_cat'] = rx_other_paid['event_attr6'].map(tx_cat)\n",
    "rx_other_paid['tx_cat'] = np.where(~rx_other_paid['MME'].isnull(),'Opioid Treatment',rx_other_paid['tx_cat'])\n",
    "\n",
    "#taking only negative day claims for analysis further\n",
    "rx_other_paid = rx_other_paid[rx_other_paid['Days'] <0]\n",
    "\n",
    "# Total claim count\n",
    "rx_other_paid0 = pd.DataFrame(rx_other_paid.groupby('id')['id'].agg('count'))\n",
    "rx_other_paid0.columns = ['claim_count']\n",
    "rx_other_paid0.reset_index(inplace=True)\n",
    "rx_other_paid0.set_index('id',inplace=True)\n",
    "\n",
    "# Total claim count by category\n",
    "rx_other_paid1 = pd.DataFrame(rx_other_paid.groupby(['id', 'tx_cat'])['id'].agg('count'))\n",
    "rx_other_paid1.columns = ['claim_count']\n",
    "rx_other_paid1.reset_index(inplace=True)\n",
    "rx_other_paid2 = pd.DataFrame(pd.pivot_table(rx_other_paid1, index = 'id', columns = 'tx_cat', values = 'claim_count')).reset_index()\n",
    "rx_other_paid2.fillna(0, inplace=True)\n",
    "rx_other_paid2.set_index('id',inplace=True)\n",
    "\n",
    "# Average gap between two claims of same category\n",
    "rx_other_paid['claim_end_day'] = rx_other_paid['Days']+rx_other_paid['PAY_DAY_SUPPLY_CNT']\n",
    "rx_other_paid3 = rx_other_paid[['id', 'tx_cat', 'Days', 'claim_end_day']]\n",
    "rx_other_paid3['same_cat_next_claim_day'] = rx_other_paid3.groupby([\"id\",\"tx_cat\"])['Days'].shift(-1)\n",
    "rx_other_paid3['next_claim_day'] = rx_other_paid3.groupby(\"id\")['Days'].shift(-1)\n",
    "rx_other_paid3['same_cat_gap'] = rx_other_paid3['same_cat_next_claim_day']-rx_other_paid3['Days']\n",
    "rx_other_paid3['gap'] = rx_other_paid3['next_claim_day']-rx_other_paid3['Days']\n",
    "rx_other_paid4 = pd.DataFrame(rx_other_paid3.groupby(['id','tx_cat'])['same_cat_gap'].mean())\n",
    "rx_other_paid4.columns = ['same_cat_avg_gap']\n",
    "rx_other_paid4.reset_index(inplace=True)\n",
    "rx_other_paid4 = pd.DataFrame(pd.pivot_table(rx_other_paid4, index = 'id', columns = 'tx_cat', values = 'same_cat_avg_gap')).reset_index()\n",
    "rx_other_paid4.fillna(0, inplace=True)\n",
    "rx_other_paid4.set_index('id',inplace=True)\n",
    "\n",
    "# Average gap between two claims overall\n",
    "rx_other_paid5 = pd.DataFrame(rx_other_paid3.groupby('id')['gap'].mean())\n",
    "rx_other_paid5.columns = ['avg_gap']\n",
    "rx_other_paid5.reset_index(inplace=True)\n",
    "rx_other_paid5.set_index('id',inplace=True)\n",
    "\n",
    "# Last claim before Day 0\n",
    "rx_other_paid['gap_from_last_claim_end'] = np.where((rx_other_paid['PAY_DAY_SUPPLY_CNT'].isnull()),0-rx_other_paid['Days'],0-(rx_other_paid['Days']+rx_other_paid['PAY_DAY_SUPPLY_CNT']))\n",
    "rx_other_paid6 = rx_other_paid.copy(deep=True)\n",
    "rx_other_paid6['id_claim_rank'] = rx_other_paid6.groupby('id')['Days'].rank(method='first', ascending=False)\n",
    "rx_other_paid6 = rx_other_paid6[rx_other_paid6['id_claim_rank'] == 1]\n",
    "rx_other_paid6.set_index('id',inplace=True)\n",
    "\n",
    "# Last Opioid claim before Day 0\n",
    "mask2 = rx_other_paid['MME'].notnull()\n",
    "rx_other_paid7 = rx_other_paid.loc[mask2]\n",
    "rx_other_paid7['opioid_claim_rank'] = rx_other_paid7.groupby('id')['Days'].rank(method='first', ascending=False)\n",
    "rx_other_paid7 = rx_other_paid7[rx_other_paid7['opioid_claim_rank'] == 1]\n",
    "rx_other_paid7.set_index('id',inplace=True)\n",
    "\n",
    "# Sum of all amounts\n",
    "rx_other_paid8 = rx_other_paid.copy(deep=True)\n",
    "rx_other_paid8['event_attr3'] = rx_other_paid8['event_attr3'].map(to_float)\n",
    "rx_other_paid8['event_attr4'] = rx_other_paid8['event_attr4'].map(to_float)\n",
    "rx_other_paid8.fillna(0, inplace=True)\n",
    "rx_other_paid9 = rx_other_paid8.groupby(['id'])[['event_attr3', 'event_attr4', 'event_attr9']].sum()\n",
    "rx_other_paid9.columns = ['tot_rx_cost', 'tot_net_paid_amt', 'tot_mem_res_amt']\n",
    "rx_other_paid9.reset_index(inplace=True)\n",
    "rx_other_paid9.set_index('id',inplace=True)\n",
    "\n",
    "\n",
    "# Rejected claims\n",
    "rx_rejected = rx[(rx['event_descr'] == 'RX Claim - Rejected')&(rx['Days']<0)]\n",
    "\n",
    "# Total rejected claim count\n",
    "rx_rejected1 = pd.DataFrame(rx_rejected.groupby('id')['id'].count())\n",
    "rx_rejected1.columns = ['rejected_claim_count']\n",
    "rx_rejected1.reset_index(inplace=True)\n",
    "rx_rejected1.set_index('id',inplace=True)\n",
    "\n",
    "\n",
    "# Total rejected claim count by reason\n",
    "rx_rejected2 = pd.DataFrame(rx_rejected.groupby(['id', 'event_attr1'])['id'].count())\n",
    "rx_rejected2.columns = ['reason_rejected_claim_count']\n",
    "rx_rejected2.reset_index(inplace = True)\n",
    "rx_rejected3 = pd.DataFrame(pd.pivot_table(rx_rejected2, index = 'id', columns = 'event_attr1', \n",
    "                                           values = 'reason_rejected_claim_count')).reset_index()\n",
    "rx_rejected3.fillna(0, inplace=True)\n",
    "\n",
    "# Total first mail order claim count\n",
    "rx_first_mail = rx[rx['event_descr'] == 'RX Claim - First Time Mail Order']\n",
    "rx_first_mail1 = pd.DataFrame(rx_first_mail.groupby('id')['id'].count())\n",
    "rx_first_mail1.columns = ['first_mail_claim_count']\n",
    "rx_first_mail1.reset_index(inplace=True)\n",
    "rx_first_mail1.set_index('id',inplace=True)\n",
    "\n",
    "# ------List of all unique id \n",
    "# ------use table : ltot instead \n",
    "# all_id = data_train.groupby('id')['id'].count().rename(columns={'id':'count'})\n",
    "# all_id = pd.DataFrame({'id':all_id.index, 'no_records':all_id.values})\n",
    "# print(len(all_id))\n",
    "\n",
    "# rx_first_mail1[:10]\n",
    "# rx_other_paid2\n",
    "# rx_other_paid5[:2]\n",
    "# rx_other_paid7[:10]\n",
    "\n",
    "\n",
    "print ('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-718239d21b2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mfeature_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'no_new_sx_flag'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Days_last_sx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mfeature_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'on_op_pre'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mfeature_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual_dos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mfeature_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Days_last_sx_bkt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Days_last_sx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mduplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mfeature_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latest_new_dx_day_bkt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latest_new_dx_day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mduplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mfeature_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'claim_end_day_bkt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'claim_end_day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mduplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36mqcut\u001b[0;34m(x, q, labels, retbins, precision, duplicates)\u001b[0m\n\u001b[1;32m    311\u001b[0m     fac, bins = _bins_to_cuts(x, bins, labels=labels,\n\u001b[1;32m    312\u001b[0m                               \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_lowest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                               dtype=dtype, duplicates=duplicates)\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     return _postprocess_for_cut(fac, bins, retbins, x_is_series,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36m_bins_to_cuts\u001b[0;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates)\u001b[0m\n\u001b[1;32m    354\u001b[0m             labels = _format_labels(bins, precision, right=right,\n\u001b[1;32m    355\u001b[0m                                     \u001b[0minclude_lowest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_lowest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                                     dtype=dtype)\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36m_format_labels\u001b[0;34m(bins, precision, right, include_lowest, dtype)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# we will adjust the left hand side by precision to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# account that we are all right closed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntervalIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/interval.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntervalArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shallow_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/interval.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3957\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3958\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# ------- Feature matrix creation\n",
    "feature_mat0 = ltot.join(latest_dx_id, on='id', how='left')\n",
    "feature_mat1 = feature_mat0.join(latest_new_dx_id, on='id',how='left')\n",
    "feature_mat2 = feature_mat1.join(feature_dx1, on='id',how='left')\n",
    "feature_mat3 = feature_mat2.join(latest_new_prov, on='id',how='left')\n",
    "feature_mat4 = feature_mat3.join(feature_data_call, on='id',how='left')\n",
    "feature_mat5 = feature_mat4.join(id_op_mat, on='id',how='left')\n",
    "feature_mat6 = feature_mat5.join(latest_sx_id, on='id',how='left')\n",
    "feature_mat7 = feature_mat6.join(rx_other_paid7['claim_end_day'], on='id',how='left')\n",
    "feature_mat8 = feature_mat7.join(rx_other_paid2, on='id',how='left')\n",
    "feature_mat9 = feature_mat8.join(rx_rejected1, on='id',how='left')\n",
    "feature_mat = feature_mat9.join(rx_other_paid9, on='id',how='left')\n",
    "\n",
    "values = {'new_dx_CPD':0,'new_dx_hypertension': 0,'new_dx_top5': 0,'new_dx_CAD': 0,'new_dx_diabetes':0,\n",
    "          'new_dx_CHF':0,'latest_new_dx_day':0,'latest_sx_desc':'unk','latest_sx_pot':'unk',\n",
    "          'Days_last_sx':0,'latest_new_prov_date':0,'call_by_mbr':0,'call_by_other':0,'call_by_prov':0,\n",
    "         'actual_dos':0,'PAY_DAY_SUPPLY_CNT':0,'PAYABLE_QTY':0,'total_rx_MME':0,'rx_cost':0,'mem_res_cost':0,\n",
    "         'claim_end_day':0,'Opioid Treatment':0,'Other Treatment':0,'Pain Treatment':0,'rejected_claim_count':0,\n",
    "         'rejection_ratio':0,'latest_dx_desc':'unk','latest_dx_pot':'unk','Days_last_dx':0,\n",
    "         'tot_charge_amt':0,'net_paid_amt':0,'mbr_res_amt':0,'tot_rx_cost':0,\n",
    "         'tot_net_paid_amt':0,'tot_mem_res_amt':0,'op_rx_cost_share':0,'LTOT_flag':0}\n",
    "\n",
    "feature_mat = feature_mat.fillna(value=values)\n",
    "feature_mat['net_mbr_res_amt'] = feature_mat['mbr_res_amt']+feature_mat['mem_res_cost']+feature_mat['tot_mem_res_amt']\n",
    "feature_mat['op_rx_cost_share'] = feature_mat['mem_res_cost']/feature_mat['tot_mem_res_amt']\n",
    "\n",
    "feature_mat['no_new_dx_flag'] = np.where(feature_mat['latest_new_dx_day'].isnull(),1,0) \n",
    "feature_mat['no_new_sx_flag'] = np.where(feature_mat['Days_last_sx'].isnull(),1,0)\n",
    "feature_mat['on_op_pre'] = np.where(~feature_mat['actual_dos'].isnull(),1,0)\n",
    "feature_mat['Days_last_sx_bkt'] = pd.qcut(feature_mat['Days_last_sx'].values, 5,duplicates='drop').codes\n",
    "feature_mat['latest_new_dx_day_bkt'] = pd.qcut(feature_mat['latest_new_dx_day'].values, 5,duplicates='drop').codes\n",
    "feature_mat['claim_end_day_bkt'] = pd.qcut(feature_mat['claim_end_day'].values, 5,duplicates='drop').codes\n",
    "feature_mat['actual_dos_bkt'] = pd.qcut(feature_mat['actual_dos'].values, 5,duplicates='drop').codes\n",
    "feature_mat['rejection_ratio'] = 1*(feature_mat['rejected_claim_count'])/(feature_mat['Opioid Treatment']+feature_mat['Other Treatment']+feature_mat['Pain Treatment'])\n",
    "# feature_mat['Days_last_dx_bkt'] = pd.qcut(feature_mat['Days_last_dx'].values, 5).codes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bins = np.array([0,5,25,50,150,250,1000,5000,10000])\n",
    "\n",
    "feature_mat_tr = feature_mat.fillna(value=values)\n",
    "\n",
    "feature_mat_tr.rename(columns={'actual_dos':'actual_dos_op_pre','PAY_DAY_SUPPLY_CNT':'dos_op_pre',\n",
    "                           'PAYABLE_QTY':'op_qty_pre','total_rx_MME':'MME','rx_cost':'tot_rx_cost_op',\n",
    "                           'mem_res_cost':'mem_res_cost_op','claim_end_day':'last_op_claim_end_day',\n",
    "                           'Opioid Treatment':'claims_op_pre','Other Treatment':'claims_oth_pre',\n",
    "                           'Pain Treatment':'claims_pain_pre','actual_dos_bkt':'actual_dos_op_pre_bkt',\n",
    "                           'tot_charge_amt':'dx_tot_charge_amt','net_paid_amt':'dx_net_paid_amt',\n",
    "                           'mbr_res_amt':'dx_mbr_res_amt','claim_end_day_bkt':'last_op_claim_end_day_bkt'},inplace=True)\n",
    "\n",
    "print(\"Column-wise missing value percentage\")\n",
    "print(feature_mat_tr.isnull().sum()/len(feature_mat_tr)*100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# feature_mat.columns#[:10]\n",
    "\n",
    "\n",
    "# feature_mat.describe()\n",
    "# feature_mat['LTOT_flag'].sum()/len(feature_mat)*100\n",
    "\n",
    "\n",
    "# feature_mat.plot.scatter(x='dx_mbr_res_amt',y='dx_tot_charge_amt',c='LTOT_flag',colormap='viridis',sharex=False)\n",
    "# xlabel('mbr_res_amt')\n",
    "# ylabel('tot_charge_amt')\n",
    "# show()\n",
    "\n",
    "# # mbr_res_amt\n",
    "# sc_var = ['claims_op_pre','claims_oth_pre','claims_pain_pre']\n",
    "# # sc_var = ['actual_dos_op_pre','dos_op_pre','op_qty_pre','MME','tot_rx_cost_op','mem_res_cost_op','mbr_res_amt','last_op_claim_end_day']\n",
    "\n",
    "# corr = feature_mat[sc_var].corr()\n",
    "# ax = sns.heatmap(\n",
    "#     corr, \n",
    "#     vmin=-1, vmax=1, center=0,\n",
    "#     cmap=sns.diverging_palette(20, 220, n=200),\n",
    "#     square=True\n",
    "# )\n",
    "# ax.set_xticklabels(\n",
    "#     ax.get_xticklabels(),\n",
    "#     rotation=90,\n",
    "#     horizontalalignment='right'\n",
    "# #     ax.set_xticks(np.arange(15)\n",
    "# );\n",
    "# show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier as sgd\n",
    "from sklearn.model_selection import cross_val_score as cvs, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, average_precision_score, precision_recall_curve, roc_curve,roc_auc_score,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from inspect import signature\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13966, 46)\n",
      "(6000, 46)\n"
     ]
    }
   ],
   "source": [
    "print (feature_mat_tr.shape)\n",
    "print (feature_mat_ho.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "value -1 for Parameter gpu_id should be greater equal to 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 514, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\", line 732, in fit\n    callbacks=callbacks)\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/xgboost/training.py\", line 216, in train\n    xgb_model=xgb_model, callbacks=callbacks)\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/xgboost/training.py\", line 74, in _train_internal\n    bst.update(dtrain, i, obj)\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 1109, in update\n    dtrain.handle))\n  File \"/Users/udbhavverma/anaconda3/lib/python3.7/site-packages/xgboost/core.py\", line 176, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: value -1 for Parameter gpu_id should be greater equal to 0\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d7c75bbbf25c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m#                           cv = 3, n_jobs = -1, verbose = 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mrf_model_rcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: value -1 for Parameter gpu_id should be greater equal to 0"
     ]
    }
   ],
   "source": [
    "dataset = feature_mat_tr#[:100]\n",
    "holdout = feature_mat_ho\n",
    "\n",
    "x_var = ['no_new_dx_flag','Days_last_sx_bkt', 'Days_last_dx',\n",
    "         'latest_new_dx_day_bkt',\n",
    "         'dx_mbr_res_amt','call_by_mbr','call_by_prov','call_by_other',\n",
    "         'actual_dos_op_pre_bkt','op_qty_pre','on_op_pre','MME','mem_res_cost_op','last_op_claim_end_day_bkt',\n",
    "        'claims_pain_pre','claims_oth_pre','claims_op_pre','rejection_ratio']\n",
    "\n",
    "X = dataset[x_var]\n",
    "Y = dataset['LTOT_flag']\n",
    "X_ho = holdout[x_var]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.40, random_state=15)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Don't cheat - fit only on training data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)  # apply same transformation to test data\n",
    "# X_ho = scaler.transform(X_ho)\n",
    "\n",
    "# print ('Train:',len(X_train),' Test:',len(X_test))\n",
    "# print (X_ho.describe())\n",
    "\n",
    "\n",
    "################### Baseline classifier\n",
    "\n",
    "class base_classifier(BaseEstimator):\n",
    "    def fit(self, X, Y):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X),1), dtype=bool)\n",
    "\n",
    "base_clf = base_classifier()\n",
    "base_accuracy = cvs(base_clf, X, Y, cv=3, scoring='accuracy')\n",
    "\n",
    "\n",
    "# ################### Building SGD Classifier\n",
    "\n",
    "sgd_clf = sgd(alpha=0.0001, average=False, class_weight=None,\n",
    "           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "           l1_ratio=0.3, learning_rate='optimal', loss='modified_huber', max_iter=10,\n",
    "           n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
    "           random_state=20, shuffle=True, tol=0.001,\n",
    "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "\n",
    "sgd_model = sgd_clf.fit(X_train,Y_train)\n",
    "sgd_accuracy = cvs(sgd_clf, X_test, Y_test, cv=3, scoring='accuracy')\n",
    "y_test_pred = cvp(sgd_clf, X_test, Y_test, cv=3)\n",
    "y_scores = cvp(sgd_clf, X_test, Y_test, cv=3,method='decision_function')\n",
    "fpr, tpr, th = roc_curve(Y_test,y_scores)\n",
    "sgd_auc = roc_auc_score(Y_test, y_test_pred)\n",
    "\n",
    "xgb_clf = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, \n",
    "                        objective=\"binary:logistic\", booster=\"gbtree\", tree_method=\"auto\",\n",
    "                        n_jobs=1, gpu_id=-1, gamma=0, min_child_weight=1, max_delta_step=0,\n",
    "                        subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, \n",
    "                        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0,\n",
    "                        missing=None)\n",
    "\n",
    "\n",
    "# -------------RandomForest\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=60, criterion=\"entropy\", max_depth=30, \n",
    "                                min_samples_split=5, min_samples_leaf=4, min_weight_fraction_leaf=0.0,\n",
    "                                max_features=\"auto\", max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None,\n",
    "                                random_state=40, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [10,50,200,500]\n",
    "# # int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [10,20,30,50,100]\n",
    "# max_depth.append(None)\n",
    "# # int(x) for x in np.linspace(10, 110, num = 11)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [2, 4, 6]\n",
    "\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf_clf, param_distributions = random_grid, \n",
    "                               n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "                                \n",
    "# param_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [25, 30, 35, 50],\n",
    "#     'max_features': ['sqrt'],\n",
    "#     'min_samples_leaf': [3, 5, 10],\n",
    "#     'min_samples_split': [3, 5, 10],\n",
    "#     'n_estimators': [50, 60, 80, 100]\n",
    "# }\n",
    "# # {'n_estimators': 50, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 30}\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = rf_clf, param_grid = param_grid, \n",
    "#                           cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "\n",
    "\n",
    "# --------train model here\n",
    "rf_model_rcv = rf_random.fit(X_train, Y_train)\n",
    "rf_model = rf_clf.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# xgb_param = rf_model_rcv.get_xgb_params()\n",
    "# extra = {'num_class': 3}\n",
    "# xgb_param.update(extra)\n",
    "\n",
    "# rf_accuracy_train = cvs(rf_model_rcv, X_train, Y_train, cv=3, scoring='accuracy')\n",
    "# rf_accuracy = cvs(rf_model_rcv, X_test, Y_test, cv=3, scoring='accuracy')\n",
    "# y_test_pred_rf = cvp(rf_model_rcv, X_test, Y_test, cv=3)\n",
    "# y_train_pred_rf = cvp(rf_model_rcv, X_train, Y_train, cv=3)\n",
    "\n",
    "# y_prob_forest = cvp(rf_model_rcv, X_test, Y_test, cv=3,method='predict_proba')\n",
    "\n",
    "y_test_pred_rf = rf_model_rcv.predict(X_test)\n",
    "y_train_pred_rf = rf_model_rcv.predict(X_train)\n",
    "y_prob_forest = rf_model_rcv.predict_proba(X_test)\n",
    "y_scores_forest = y_prob_forest[:,1]\n",
    "\n",
    "y_prob_forest_ho = rf_model_rcv.predict_proba(X_ho)\n",
    "y_scores_forest_ho = y_prob_forest_ho[:,1]\n",
    "\n",
    "y_prob_forest_train = rf_model_rcv.predict_proba(X_train)\n",
    "y_scores_forest_train = y_prob_forest_train[:,1]\n",
    "\n",
    "\n",
    "fpr_rf_test, tpr_rf_test, th_rf_test = roc_curve(Y_test,y_scores_forest)\n",
    "rf_auc_test = roc_auc_score(Y_test, y_test_pred_rf)\n",
    "fpr_rf_train, tpr_rf_train, th_rf_train = roc_curve(Y_train,y_scores_forest_train)\n",
    "rf_auc_train = roc_auc_score(Y_train, y_train_pred_rf)\n",
    "\n",
    "print (rf_model_rcv.best_params_)\n",
    "\n",
    "# ---------------RandomForest on entire data\n",
    "# rf_model_all = rf_clf.fit(X,Y)\n",
    "rf_accuracy_all = cvs(rf_model, X, Y, cv=3, scoring='accuracy')\n",
    "y_all_pred_rf = cvp(rf_model, X, Y, cv=3)\n",
    "y_prob_forest_all = cvp(rf_model, X, Y, cv=3,method='predict_proba')\n",
    "y_scores_forest_all = y_prob_forest_all[:,1]\n",
    "fpr_rfall, tpr_rfall, th_rfall = roc_curve(Y,y_scores_forest_all)\n",
    "rf_auc_all = roc_auc_score(Y, y_all_pred_rf)\n",
    "\n",
    "y_prob_forest_ho_rf_clf = rf_model.predict_proba(X_ho)\n",
    "y_scores_forest_ho_rf_clf = y_prob_forest_ho_rf_clf[:,1]\n",
    "\n",
    "\n",
    "\n",
    "# ------------------GradientBoost\n",
    "# gbm_clf = GradientBoostingClassifier(loss=\"deviance\", learning_rate=0.1, n_estimators=1000, \n",
    "#                                      subsample=1.0, criterion=\"friedman_mse\", min_samples_split=2, \n",
    "#                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "#                                      min_impurity_decrease=0.0, min_impurity_split=None, init=None,\n",
    "#                                      random_state=None, max_features=None, verbose=0, max_leaf_nodes=None,\n",
    "#                                      warm_start=False, presort=\"auto\", validation_fraction=0.1,\n",
    "#                                      n_iter_no_change=None, tol=0.0001)\n",
    "\n",
    "# gbm_model = gbm_clf.fit(X_train,Y_train)\n",
    "# gbm_accuracy_train = cvs(gbm_model, X_train, Y_train, cv=3, scoring='accuracy')\n",
    "# gbm_accuracy_test = cvs(gbm_model, X_test, Y_test, cv=3, scoring='accuracy')\n",
    "# y_test_pred_gbm = cvp(gbm_model, X_test, Y_test, cv=3)\n",
    "# y_test_prob_gbm = cvp(gbm_model, X_test, Y_test, cv=3,method='predict_proba')\n",
    "# y_test_scores_gbm = y_test_prob_gbm[:,1]\n",
    "# fpr_gbm, tpr_gbm, th_gbm = roc_curve(Y_test,y_test_scores_gbm)\n",
    "# gbm_auc = roc_auc_score(Y_test, y_test_pred_gbm)\n",
    "\n",
    "# gbm_model = gbm_clf.fit(X,Y)\n",
    "# gbm_accuracy_train = cvs(gbm_model, X, Y, cv=3, scoring='accuracy')\n",
    "# gbm_accuracy_test = cvs(gbm_model, X, Y, cv=3, scoring='accuracy')\n",
    "# y_test_pred_gbm = cvp(gbm_model, X, Y, cv=3)\n",
    "# y_test_prob_gbm = cvp(gbm_model, X, Y, cv=3,method='predict_proba')\n",
    "# y_test_scores_gbm = y_test_prob_gbm[:,1]\n",
    "# fpr_gbm, tpr_gbm, th_gbm = roc_curve(Y,y_test_scores_gbm)\n",
    "# gbm_auc = roc_auc_score(Y, y_test_pred_gbm)\n",
    "\n",
    "\n",
    "# ---------ROC curve\n",
    "def plot_roc_curve(fpr,tpr,label=None):\n",
    "    plt.plot(fpr,tpr,linewidth=2,label=label)\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.xlabel('False Positive Rate : FP/FP+TN')\n",
    "    plt.ylabel('True Positive Rate : TP/TP+FN')\n",
    "    \n",
    "plt.plot(fpr_rf_train,tpr_rf_train,\"b:\",label=\"XGB-train auto tuned\")\n",
    "plot_roc_curve(fpr_rf_test,tpr_rf_test,\"XGB-test auto tuned\")\n",
    "plot_roc_curve(fpr_rfall,tpr_rfall,\"RF-clf\")\n",
    "# plot_roc_curve(fpr_gbm,tpr_gbm,\"GBM test\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# ----------- model evaluation\n",
    "print ('rf accuracy all (TP+TN/n): ',rf_accuracy_all.mean(),'|rf accuracy Train: ',rf_accuracy_train.mean(),'|rf accuracy Test: ',rf_accuracy.mean())\n",
    "print ('rf accuracy (TP+TN/n): ',rf_accuracy.mean())\n",
    "print ('sgd accuracy (TP+TN/n: ',sgd_accuracy.mean())\n",
    "print ('base accuracy (TP+TN/n: ',base_accuracy.mean())\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "cm_rf = confusion_matrix(Y_test, y_test_pred_rf)\n",
    "cm_rf_all = confusion_matrix(Y, y_all_pred_rf)\n",
    "index = ['NON LTOT','LTOT']  \n",
    "columns = ['NON LTOT','LTOT']    \n",
    "cm_df = pd.DataFrame(cm,columns,index)\n",
    "cm_df_rf = pd.DataFrame(cm_rf,columns,index)\n",
    "# print ('SGD')\n",
    "# plt.figure(figsize=(5,3))  \n",
    "# sns.heatmap(cm_df,annot=True,fmt='g')\n",
    "# show()\n",
    "# print ('RF')\n",
    "# plt.figure(figsize=(5,3))  \n",
    "# sns.heatmap(cm_rf,annot=True,fmt='g')\n",
    "# show()\n",
    "print ('RF all')\n",
    "plt.figure(figsize=(5,3))  \n",
    "sns.heatmap(cm_rf_all,annot=True,fmt='g')\n",
    "show()\n",
    "print(classification_report(Y, y_all_pred_rf)) ##, target_names=target_names\n",
    "\n",
    "# cm_gbm = confusion_matrix(Y, y_test_pred_gbm)\n",
    "# print('GBM')\n",
    "# plt.figure(figsize=(5,3))  \n",
    "# sns.heatmap(cm_gbm,annot=True,fmt='g')\n",
    "# show()\n",
    "\n",
    "# print ('gbm accuracy Train: ',gbm_accuracy_train.mean(),'|gbm accuracy Test: ',gbm_accuracy_test.mean())\n",
    "# print(classification_report(Y, y_test_pred_gbm)) ##, target_names=target_names\n",
    "# print ('gbm_auc=',gbm_auc)\n",
    "print('train auc auto tuned=',rf_auc_train)\n",
    "print('test auc auto tuned=',rf_auc_test)\n",
    "print('rf_auc rf_clf=',rf_auc_all)\n",
    "\n",
    "feature_importances = pd.DataFrame(rf_model.feature_importances_,\n",
    "                                   index = x_var,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "print (feature_importances)\n",
    "\n",
    "# ---------summary final model on entire data \n",
    "# df = X_ho\n",
    "# df['y_pred_rf'] = y_scores_forest\n",
    "# df['y_pred_gbm'] = y_test_pred_gbm\n",
    "\n",
    "def create_label(row,col):\n",
    "    if row['LTOT_flag'] == row[col] and row[col]==1:\n",
    "        return 'TP'\n",
    "    elif row['LTOT_flag'] == row[col] and row[col]==0:\n",
    "        return 'TN'\n",
    "    elif row['LTOT_flag'] != row[col] and row[col]==0:\n",
    "        return 'FN'\n",
    "    else:\n",
    "        return 'FP'\n",
    "\n",
    "# df['label_rf'] = df.apply (lambda row: create_label(row,'y_pred_rf'), axis=1)\n",
    "# df['label_gbm'] = df.apply (lambda row: create_label(row,'y_pred_gbm'), axis=1)\n",
    "\n",
    "\n",
    "# summary_var = ['label','dos_op_pre','MME']\n",
    "# claims_oth_pre           0.101260\n",
    "# rejected_claim_count     0.091630\n",
    "# MME                      0.090429\n",
    "# call_by_mbr              0.080635\n",
    "# mem_res_cost_op          0.066694\n",
    "# last_op_claim_end_day    0.060812\n",
    "# claims_op_pre            0.058836\n",
    "# Days_last_sx_bkt         0.037470\n",
    "# latest_new_dx_day_bkt    0.025136\n",
    "# no_new_sx_flag           0.008547\n",
    "# on_op_pre                0.007803\n",
    "# new_dx_CPD               0.007772\n",
    "# new_dx_hypertension      0.007461\n",
    "# no_new_dx_flag           0.007335\n",
    "# new_dx_diabetes          0.006155\n",
    "# new_dx_CAD               0.005924\n",
    "# new_dx_CHF               0.005332\n",
    "# df[summary_var].groupby('label').describe()\n",
    "\n",
    "# df.iloc[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=feature_mat_ho\n",
    "df['LTOT_prob_rf'] = y_scores_forest_ho_rf_clf\n",
    "df['rank'] = df['LTOT_prob_rf'].rank(method='dense',ascending=False)\n",
    "df['rank'].describe()\n",
    "df.to_csv('holdoutset_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13966, 46)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-10fd9d688843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                how='left')\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_p2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mall_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_p2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id_d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LTOT_flag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------New patient df creation\n",
    "data = data_train\n",
    "# data = data[data['id']=='ID10010854159']\n",
    "data['Days_str'] = data['Days'].map(to_str)\n",
    "data['key'] = data['id']+data['Days_str']\n",
    "data = data.sort_values(['id','Days','event_category'])\n",
    "\n",
    "# rx_op_claim[rx_op_claim['new_patient_flag']==1]['Days'].hist()\n",
    "op_new_pat = rx_op_claim[rx_op_claim['new_patient_flag']==1][['id','Days','new_patient_flag']]\n",
    "op_new_pat['initiation_rank'] = op_new_pat.groupby(['id'])['Days'].rank(method='dense')\n",
    "# op_new_pat[op_new_pat['initiation_rank']==1]['Days'].hist()\n",
    "# show()\n",
    "op_new_pat['Days_str'] = op_new_pat['Days'].map(to_str)\n",
    "op_new_pat['key'] = op_new_pat['id']+op_new_pat['Days_str']\n",
    "\n",
    "data_p1 = data.merge(op_new_pat,\n",
    "               left_on='key',\n",
    "               right_on='key',\n",
    "               how='left',\n",
    "               suffixes=['_d', '_I'])\n",
    "ltot = rx_op_episode.groupby(['id'])['LTOT_flag'].max()\n",
    "ltot = pd.DataFrame({'id':ltot.index, 'LTOT_flag':ltot.values})\n",
    "# len(ltot[ltot['LTOT_flag'].isnull()])\n",
    "# 149\n",
    "ltot['LTOT_flag'] = ltot['LTOT_flag'].fillna(0)\n",
    "ltot.set_index('id')\n",
    "\n",
    "data_p2 = data_p1.merge(ltot,\n",
    "               left_on='id_d',\n",
    "               right_on='id',\n",
    "               how='left')\n",
    "print(data_p2.shape())\n",
    "\n",
    "all_id = pd.DataFrame(data_p2.groupby('id_d')['LTOT_flag'].max())\n",
    "all_id[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------All id with LTOT flag\n",
    "data_train['id'] = data_train['id'].map(to_str)\n",
    "rx_op_episode['id'] = rx_op_episode['id'].map(to_str)\n",
    "\n",
    "ltot = rx_op_episode.groupby(['id'])['LTOT_flag'].max()\n",
    "ltot = pd.DataFrame({'id':ltot.index, 'LTOT_flag':ltot.values})\n",
    "# 13966 patients\n",
    "all_id = data_train.groupby('id')['id'].count().rename(columns={'id':'count'})\n",
    "all_id = pd.DataFrame({'id':all_id.index, 'no_records':all_id.values})\n",
    "\n",
    "print(all_id.dtypes)\n",
    "print(ltot.dtypes)\n",
    "\n",
    "print(len(all_id))\n",
    "print(len(ltot))\n",
    "\n",
    "# all_id = all_id.join(ltot, on='id', how='left')\n",
    "# all_id.fillna(value={'LTOT_flag':0})\n",
    "# print(all_id['LTOT_flag'].value_counts())\n",
    "\n",
    "# pd.merge(df1.assign(x=df1.x.astype(str)), \n",
    "#          df2.assign(x=df2.x.astype(str)), \n",
    "#          how='left', on='x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = data_train[(data_train['event_category'] == 'Diagnosis') | (data_train['event_category'] == 'Claim')]\n",
    "dx.shape\n",
    "\n",
    "# ##comment below line\n",
    "# dx = dx[:10000]\n",
    "\n",
    "def to_float(s):\n",
    "    return float(s)\n",
    "\n",
    "dx['charge_amt'] = dx['event_attr3'].map(to_float)\n",
    "dx['net_amt_paid'] = dx['event_attr4'].map(to_float)\n",
    "dx['member_res_amt'] = dx['event_attr5'].map(to_float)\n",
    "dx_pat_att = dx.groupby(['id'])[['charge_amt','net_amt_paid','member_res_amt']].sum()\n",
    "pat_len = data_train.groupby('id').apply(lambda x: x.Days.max() - x.Days.min())\n",
    "pat_len = pd.DataFrame({'id':pat_len.index, 'len':pat_len.values})\n",
    "dx_pat = dx_pat_att.join(pat_len.set_index('id'), on='id')\n",
    "\n",
    "dx_pat['norm_charge_amt'] = dx_pat['charge_amt']/dx_pat['len']\n",
    "dx_pat['norm_net_amt_paid'] = dx_pat['net_amt_paid']/dx_pat['len']\n",
    "dx_pat['norm_member_res_amt'] = dx_pat['member_res_amt']/dx_pat['len']\n",
    "\n",
    "# plot(dx_pat['len'], dx_pat['norm_member_res_amt'], marker='o', color='blue', linestyle='None')\n",
    "# xlabel('len')\n",
    "# ylabel('norm_member_res_amt')\n",
    "#show()\n",
    "# ## ID10166893764\n",
    "\n",
    "# ##Types of diagnosis\n",
    "dx_pat_diag = dx.groupby(['id','event_descr'])[['event_descr']].count().rename(columns={'event_descr':'count'})\n",
    "dx_pat_diag.reset_index(inplace=True)\n",
    "dx_pat_diag_pv = dx_pat_diag.pivot_table(index='id', columns='event_descr', values='count', fill_value=0)\n",
    "dx_pat_diag_pv.shape\n",
    "\n",
    "# dx_pat = dx_pat.join(dx_pat_diag_pv, on='id')\n",
    "# dx_pat['norm_dx_claim_count'] = dx_pat['Fully Paid Claim']/dx_pat['len']\n",
    "\n",
    "# plot(dx_pat['Surgery'], dx_pat['norm_dx_claim_count'], marker='o', color='blue', linestyle='None')\n",
    "# xlabel('Surgery')\n",
    "# ylabel('norm_dx_claim_count')\n",
    "# show()\n",
    "\n",
    "# print (dx_pat['Surgery'].min())\n",
    "# print (dx_pat['Surgery'].max())\n",
    "\n",
    "# dx_pat['Surgery'].value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying LTOT patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating pre/post flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 400\n",
    "\n",
    "def to_str(s):\n",
    "    return str(s)\n",
    "\n",
    "\n",
    "# ## New patient df creation\n",
    "data = data_train\n",
    "# data = data[data['id']=='ID10010854159']\n",
    "data['Days_str'] = data['Days'].map(to_str)\n",
    "data['key'] = data['id']+data['Days_str']\n",
    "data = data.sort_values(['id','Days','event_category'])\n",
    "\n",
    "# rx_op_claim[rx_op_claim['new_patient_flag']==1]['Days'].hist()\n",
    "op_new_pat = rx_op_claim[rx_op_claim['new_patient_flag']==1][['id','Days','new_patient_flag']]\n",
    "op_new_pat['initiation_rank'] = op_new_pat.groupby(['id'])['Days'].rank(method='dense')\n",
    "# op_new_pat[op_new_pat['initiation_rank']==1]['Days'].hist()\n",
    "# show()\n",
    "op_new_pat['Days_str'] = op_new_pat['Days'].map(to_str)\n",
    "op_new_pat['key'] = op_new_pat['id']+op_new_pat['Days_str']\n",
    "\n",
    "data_p1 = data.merge(op_new_pat,\n",
    "               left_on='key',\n",
    "               right_on='key',\n",
    "               how='left',\n",
    "               suffixes=['_d', '_I'])\n",
    "ltot = rx_op_episode.groupby(['id'])['LTOT_flag'].max()\n",
    "ltot = pd.DataFrame({'id':ltot.index, 'LTOT_flag':ltot.values})\n",
    "# len(ltot[ltot['LTOT_flag'].isnull()])\n",
    "# 149\n",
    "ltot['LTOT_flag'] = ltot['LTOT_flag'].fillna(0)\n",
    "ltot.set_index('id')\n",
    "\n",
    "data_p2 = data_p1.merge(ltot,\n",
    "               left_on='id_d',\n",
    "               right_on='id',\n",
    "               how='left')\n",
    "data_p2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_p2[data_p2['LTOT_flag'].isnull()]#['id']#.drop_duplicates()#.count()\n",
    "\n",
    "ltot1 = rx_op_episode.groupby(['id'])['LTOT_flag'].max()\n",
    "ltot1 = pd.DataFrame({'id':ltot1.index, 'LTOT_flag':ltot1.values})\n",
    "# len(ltot[ltot['LTOT_flag'].isnull()])\n",
    "# 149\n",
    "\n",
    "\n",
    "\n",
    "# ID10433575841\n",
    "# ID1624200167\n",
    "# rx_op_episode[rx_op_episode['id']=='ID10433575841']\n",
    "\n",
    "# opioid=['OPIOID COMBINATIONS','OPIOID AGONISTS','OPIOID PARTIAL AGONISTS','ANTIPERISTALTIC AGENTS']\n",
    "\n",
    "# len(data_p1[(data_p1['event_descr']=='RX Claim - Paid')&(data_p1['event_attr1'].isin(opioid))]['id_d'].drop_duplicates())\n",
    "\n",
    "# # data_p1[(data_p1['event_descr']=='RX Claim - Paid')&(~data_p1['MME'].isnull())]['event_attr1'].value_counts()\n",
    "\n",
    "# #['event_attr1'].value_counts()\n",
    "\n",
    "# ltot1[ltot1['LTOT_flag'].isnull()]\n",
    "# data_p1[(data_p1['id_d']=='ID1624200167')][['MME','PAY_DAY_SUPPLY_CNT','event_descr','event_attr1','Days_d']]#.min()\n",
    "# data_p2\n",
    "# # print(\"Column-wise missing value percentage\")\n",
    "# print(data_p2.isnull().sum()/len(data_p2)*100)\n",
    "\n",
    "# data_p2['LTOT_flag'] = np.where(data_p2['LTOT_flag'].isnull(),0,data_p2['LTOT_flag'])\n",
    "\n",
    "# print(\"Column-wise missing value percentage\")\n",
    "# print(data_p2.isnull().sum()/len(data_p2)*100)\n",
    "feature_mat.shape\n",
    "\n",
    "# d = data_p2.groupby('id')['LTOT_flag'].max()\n",
    "all_id = pd.DataFrame(data_p2.groupby('id')['LTOT_flag'].max())\n",
    "all_id[:10]\n",
    "# all_id.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## coding parameters\n",
    "# import re\n",
    "warnings.filterwarnings('ignore')\n",
    "col = ['id','event_descr','event_attr1','event_attr2','event_attr3','event_attr4','event_attr5','Days_d']\n",
    "drop_col = ['event_attr1','event_attr2','event_attr3','event_attr4','event_attr5','event_attr6','event_attr7','event_attr8']\n",
    "\n",
    "# ################################\n",
    "# Feature extraction:\n",
    "# Pre/Post-\n",
    "# Total number of opioid episodes\n",
    "# total length of episode:sum(episode_len)\n",
    "# total length of therapy:max(episode_end)-min(episode_start)\n",
    "\n",
    "# Gap from last Opioid before initiation : \n",
    "    \n",
    "# ################################\n",
    "\n",
    "data_dx = data_p2[(data_p2['event_category']=='Claim')&(~data_p2['LTOT_flag'].isnull())]\n",
    "data_dx['pre_flag'] = np.where(data_dx['Days_d']<=0,1,0)\n",
    "\n",
    "\n",
    "# ------- Opioid intake related features\n",
    "aggregation = {'actual_dos':sum,'PAY_DAY_SUPPLY_CNT':sum,'PAYABLE_QTY':sum,'total_rx_MME':sum,\n",
    "              'rx_cost':sum,'mem_res_cost':sum}        \n",
    "id_op_mat = rx_op_episode.groupby(['id','category']).agg(aggregation)\n",
    "id_op_mat.reset_index(inplace=True)\n",
    "id_op_mat = id_op_mat[id_op_mat['category']=='pre']\n",
    "id_op_mat.drop('category',axis=1,inplace=True)\n",
    "id_op_mat.set_index('id',inplace=True)\n",
    "id_op_mat.shape\n",
    "\n",
    "# ------ Latest Dx before initiation (day=0) and days since last Dx\n",
    "data_dx_pre = data_dx[(data_dx['pre_flag']==1)&(data_dx['event_descr']=='Fully Paid Claim')]\n",
    "# data_dx_pre = data_dx_pre[data_dx_pre['id_d']== 'ID71291582272'] \n",
    "idx = data_dx_pre.groupby(['id'])['Days_d'].transform(max) == data_dx_pre['Days_d']\n",
    "latest_dx = data_dx_pre[idx]        \n",
    "latest_dx1 = latest_dx.groupby(['id_d','event_attr1','event_attr2','Days_d','LTOT_flag'])[['id_d']].count().rename(columns={'id_d':'count'})\n",
    "latest_dx1.reset_index(inplace=True)\n",
    "latest_dx1['rank_count'] = latest_dx1.groupby(['id_d'])['count'].rank(method='first',ascending=False)\n",
    "latest_dx_id = latest_dx1[latest_dx1['rank_count']==1]\n",
    "latest_dx_id = latest_dx_id[['id_d','event_attr1','event_attr2','Days_d']].rename(columns={'event_attr1':'latest_dx_desc','event_attr2':'latest_dx_pot','id_d':'id','Days_d':'Days_last_dx'})\n",
    "latest_dx_id.set_index('id',inplace=True)\n",
    "\n",
    "# ------ other Dx features\n",
    "data_dx_pre['tot_charge_amt'] = data_dx_pre['event_attr3'].map(to_float)\n",
    "data_dx_pre['net_paid_amt'] = data_dx_pre['event_attr4'].map(to_float)\n",
    "data_dx_pre['mbr_res_amt'] = data_dx_pre['event_attr5'].map(to_float)\n",
    "data_dx_pre['mbr_res_amt'] = np.where(data_dx_pre['mbr_res_amt']<0,0,data_dx_pre['mbr_res_amt'])\n",
    "feature_dx1 = data_dx_pre.groupby(['id_d'])[['tot_charge_amt','net_paid_amt','mbr_res_amt']].sum()\n",
    "feature_dx1 = feature_dx1.reset_index().rename(columns={'id_d':'id'})\n",
    "feature_dx1.set_index('id',inplace=True)\n",
    "\n",
    "# feature_dx1.plot.scatter(x='net_paid_amt',y='mbr_res_amt',c='LTOT_flag',colormap='viridis',sharex=False)\n",
    "# feature_dx1.hist(bins=25, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=1.5)\n",
    "# show()\n",
    "feature_dx1.describe()\n",
    "\n",
    "# ------ latest New Dx before initiation (day=0) and days since last new Dx\n",
    "data_new_dx = data_p2[(data_p2['event_category']=='Diagnosis')&(~data_p2['LTOT_flag'].isnull())]\n",
    "data_new_dx['pre_flag'] = np.where(data_new_dx['Days_d']<=0,1,0)\n",
    "data_new_dx_pre = data_new_dx[(data_new_dx['pre_flag']==1)]\n",
    "nidx = data_new_dx_pre.groupby(['id'])['Days_d'].transform(max) == data_new_dx_pre['Days_d']\n",
    "latest_new_dx = data_new_dx_pre[nidx]  \n",
    "latest_new_dx['new_dx_CPD'] = np.where(latest_new_dx['event_descr']=='New diagnosis - CPD',1,0)\n",
    "latest_new_dx['new_dx_hypertension'] = np.where(latest_new_dx['event_descr']=='New diagnosis - Hypertension',1,0)\n",
    "latest_new_dx['new_dx_top5'] = np.where(latest_new_dx['event_descr']=='New diagnosis - Top 5',1,0)\n",
    "latest_new_dx['new_dx_CAD'] = np.where(latest_new_dx['event_descr']=='New diagnosis - CAD',1,0)\n",
    "latest_new_dx['new_dx_diabetes'] = np.where(latest_new_dx['event_descr']=='New diagnosis - Diabetes',1,0)\n",
    "latest_new_dx['new_dx_CHF'] = np.where(latest_new_dx['event_descr']=='New diagnosis - CHF',1,0)\n",
    "latest_new_dx_id = latest_new_dx.groupby(['id','Days_d'])['new_dx_CPD','new_dx_hypertension','new_dx_top5','new_dx_CAD','new_dx_diabetes','new_dx_CHF'].max()\n",
    "latest_new_dx_id = latest_new_dx_id.reset_index().rename(columns={'id_d':'id','Days_d':'latest_new_dx_day'})\n",
    "latest_new_dx_id.set_index('id',inplace=True)\n",
    "latest_new_dx_id.describe()\n",
    "# latest_new_dx_id[:5]\n",
    "# 5711 patients with new dx\n",
    "\n",
    "# ------- calls related features\n",
    "data_call = data_p2[(data_p2['event_category']=='Call')&(~data_p2['LTOT_flag'].isnull())]\n",
    "data_call['pre_flag'] = np.where(data_call['Days_d']<=0,1,0)\n",
    "data_call_pre = data_call[data_call['pre_flag']==1]\n",
    "data_call_pre_id = data_call_pre.groupby(['id','LTOT_flag','event_descr'])[['id']].count().rename(columns={'id':'number_calls'})\n",
    "data_call_pre_id.reset_index(inplace=True)\n",
    "data_call_pre_id['call_by_mbr'] = np.where(data_call_pre_id['event_descr']=='Inbound Call by Mbr',data_call_pre_id['number_calls'],0)\n",
    "data_call_pre_id['call_by_prov'] = np.where(data_call_pre_id['event_descr']=='Inbound Call by Prov',data_call_pre_id['number_calls'],0)\n",
    "data_call_pre_id['call_by_other'] = np.where(data_call_pre_id['event_descr']=='Inbound Call by Other',data_call_pre_id['number_calls'],0)\n",
    "feature_data_call = data_call_pre_id.groupby(['id'])['call_by_mbr','call_by_prov','call_by_other'].max()\n",
    "feature_data_call = feature_data_call.reset_index()\n",
    "feature_data_call.set_index('id',inplace=True)\n",
    "# feature_data_call[:5]\n",
    "\n",
    "# ------- provider related features\n",
    "data_prov = data_p2[(data_p2['event_category']=='Provider')&(~data_p2['LTOT_flag'].isnull())]\n",
    "data_prov['pre_flag'] = np.where(data_prov['Days_d']<=0,1,0)\n",
    "data_prov_pre = data_prov[data_prov['pre_flag']==1]\n",
    "pidx = data_prov_pre.groupby(['id'])['Days_d'].transform(max) == data_prov_pre['Days_d']\n",
    "latest_new_prov = data_prov_pre[pidx]  \n",
    "latest_new_prov = latest_new_prov[['id','Days_d']].rename(columns={'Days_d':'latest_new_prov_date'})\n",
    "# latest_new_prov = latest_new_prov.reset_index()\n",
    "latest_new_prov.set_index('id',inplace=True)\n",
    "\n",
    "# ------ Latest Surgery before initiation (day=0) and days since last surgery\n",
    "data_sx_pre = data_dx[(data_dx['pre_flag']==1)&(data_dx['event_descr']=='Surgery')]\n",
    "# data_sx_pre = data_sx_pre[data_sx_pre['id_d']== 'ID99421949451'] \n",
    "idsx = data_sx_pre.groupby(['id'])['Days_d'].transform(max) == data_sx_pre['Days_d']\n",
    "latest_sx = data_sx_pre[idsx]\n",
    "\n",
    "latest_sx1 = latest_sx.groupby(['id_d','event_attr1','event_attr2','Days_d','LTOT_flag'])[['id_d']].count().rename(columns={'id_d':'count'})\n",
    "latest_sx1.reset_index(inplace=True)\n",
    "latest_sx1['rank_count'] = latest_sx1.groupby(['id_d'])['count'].rank(method='first',ascending=False)\n",
    "latest_sx_id = latest_sx1[latest_sx1['rank_count']==1]\n",
    "latest_sx_id = latest_sx_id[['id_d','event_attr1','event_attr2','Days_d']].rename(columns={'event_attr1':'latest_sx_desc','event_attr2':'latest_sx_pot','id_d':'id','Days_d':'Days_last_sx'})\n",
    "latest_sx_id.set_index('id',inplace=True)\n",
    "\n",
    "print ('done')\n",
    "# # --------------------\n",
    "# print ('LTOT Patient')\n",
    "# latest_sx_id[latest_sx_id['LTOT_flag']==1]['Days_last_sx'].hist()\n",
    "# xlabel('Days since last surgery')\n",
    "# ylabel('# Patients')\n",
    "# show()\n",
    "# latest_dx_id[latest_dx_id['LTOT_flag']==1]['Days_last_dx'].hist()\n",
    "# xlabel('Days since last dx')\n",
    "# ylabel('# Patients')\n",
    "# show()\n",
    "\n",
    "# print ('Non LTOT Patient')\n",
    "# latest_sx_id[latest_sx_id['LTOT_flag']==0]['Days_last_sx'].hist()\n",
    "# xlabel('Days since last surgery')\n",
    "# ylabel('# Patients')\n",
    "# show()\n",
    "# latest_dx_id[latest_dx_id['LTOT_flag']==0]['Days_last_dx'].hist()\n",
    "# xlabel('Days since last dx')\n",
    "# ylabel('# Patients')\n",
    "# show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# len(data_sx_pre)\n",
    "# print (len(latest_sx1['id_d'].drop_duplicates()))\n",
    "# latest_sx_id[:10]#[col]\n",
    "\n",
    "# --------------------\n",
    "# data_dx['event_descr'].value_counts()\n",
    "# Fully Paid Claim    1731058\n",
    "# Surgery               30937\n",
    "\n",
    "# input_str = 'ALLERGIC RHINITIS, CAUSE UNSPECIFIED'\n",
    "# def check_pain(input_str):\n",
    "#     if any(re.findall(r'pain|hypertension|ALLERGIC', input_str, re.IGNORECASE)):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "# check_pain(input_str)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tx_cat(s):\n",
    "    if s in('PAIN','OTH-STEROIDS','PSYCH-ANX','PSYCH-DEP','NUISANCE-SLEEP','PSYCH','NUISANCE-STIMULANTS',\n",
    "            'CARDIO', 'CANCER', 'OTH-THYROID','OTH-GROWTH HORMONE'):\n",
    "        return 'Pain Treatment'\n",
    "    else:\n",
    "        return 'Other Treatment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rx_other_paid = rx[rx['event_descr'] == 'RX Claim - Paid']\n",
    "\n",
    "rx_other_paid['tx_cat'] = rx_other_paid['event_attr6'].map(tx_cat)\n",
    "rx_other_paid['tx_cat'] = np.where(~rx_other_paid['MME'].isnull(),'Opioid Treatment',rx_other_paid['tx_cat'])\n",
    "\n",
    "# np.where(rx_other_paid['MME'].notnull(), 'Opioid Treatment', rx_other_paid['event_attr6'].map(tx_cat))\n",
    "\n",
    "# rx_other_paid = pd.DataFrame(rx_other_paid[['id', 'event_attr1', 'event_attr3',\n",
    "#                                              'event_attr4', 'event_attr5', 'event_attr6','event_attr8',\n",
    "#                                              'event_attr9', 'event_attr10', 'Days','PAY_DAY_SUPPLY_CNT', 'PAYABLE_QTY', \n",
    "#                                              'MME', 'Specialty','Specialty2', 'Specialty3', 'event_category', 'tx_cat']])\n",
    "\n",
    "#taking only negative day claims for analysis further\n",
    "rx_other_paid = rx_other_paid[rx_other_paid['Days'] <0]\n",
    "\n",
    "# Total claim count\n",
    "rx_other_paid0 = pd.DataFrame(rx_other_paid.groupby('id')['id'].agg('count'))\n",
    "rx_other_paid0.columns = ['claim_count']\n",
    "rx_other_paid0.reset_index(inplace=True)\n",
    "rx_other_paid0.set_index('id',inplace=True)\n",
    "\n",
    "# Total claim count by category\n",
    "rx_other_paid1 = pd.DataFrame(rx_other_paid.groupby(['id', 'tx_cat'])['id'].agg('count'))\n",
    "rx_other_paid1.columns = ['claim_count']\n",
    "rx_other_paid1.reset_index(inplace=True)\n",
    "rx_other_paid2 = pd.DataFrame(pd.pivot_table(rx_other_paid1, index = 'id', columns = 'tx_cat', values = 'claim_count')).reset_index()\n",
    "rx_other_paid2.fillna(0, inplace=True)\n",
    "rx_other_paid2.set_index('id',inplace=True)\n",
    "\n",
    "# Average gap between two claims of same category\n",
    "rx_other_paid['claim_end_day'] = rx_other_paid['Days']+rx_other_paid['PAY_DAY_SUPPLY_CNT']\n",
    "rx_other_paid3 = rx_other_paid[['id', 'tx_cat', 'Days', 'claim_end_day']]\n",
    "rx_other_paid3['same_cat_next_claim_day'] = rx_other_paid3.groupby([\"id\",\"tx_cat\"])['Days'].shift(-1)\n",
    "rx_other_paid3['next_claim_day'] = rx_other_paid3.groupby(\"id\")['Days'].shift(-1)\n",
    "rx_other_paid3['same_cat_gap'] = rx_other_paid3['same_cat_next_claim_day']-rx_other_paid3['Days']\n",
    "rx_other_paid3['gap'] = rx_other_paid3['next_claim_day']-rx_other_paid3['Days']\n",
    "rx_other_paid4 = pd.DataFrame(rx_other_paid3.groupby(['id','tx_cat'])['same_cat_gap'].mean())\n",
    "rx_other_paid4.columns = ['same_cat_avg_gap']\n",
    "rx_other_paid4.reset_index(inplace=True)\n",
    "rx_other_paid4 = pd.DataFrame(pd.pivot_table(rx_other_paid4, index = 'id', columns = 'tx_cat', values = 'same_cat_avg_gap')).reset_index()\n",
    "rx_other_paid4.fillna(0, inplace=True)\n",
    "rx_other_paid4.set_index('id',inplace=True)\n",
    "\n",
    "# Average gap between two claims overall\n",
    "rx_other_paid5 = pd.DataFrame(rx_other_paid3.groupby('id')['gap'].mean())\n",
    "rx_other_paid5.columns = ['avg_gap']\n",
    "rx_other_paid5.reset_index(inplace=True)\n",
    "rx_other_paid5.set_index('id',inplace=True)\n",
    "\n",
    "# Last claim before Day 0\n",
    "rx_other_paid['gap_from_last_claim_end'] = np.where((rx_other_paid['PAY_DAY_SUPPLY_CNT'].isnull()),0-rx_other_paid['Days'],0-(rx_other_paid['Days']+rx_other_paid['PAY_DAY_SUPPLY_CNT']))\n",
    "rx_other_paid6 = rx_other_paid.copy(deep=True)\n",
    "rx_other_paid6['id_claim_rank'] = rx_other_paid6.groupby('id')['Days'].rank(method='first', ascending=False)\n",
    "rx_other_paid6 = rx_other_paid6[rx_other_paid6['id_claim_rank'] == 1]\n",
    "rx_other_paid6.set_index('id',inplace=True)\n",
    "\n",
    "# Last Opioid claim before Day 0\n",
    "mask2 = rx_other_paid['MME'].notnull()\n",
    "rx_other_paid7 = rx_other_paid.loc[mask2]\n",
    "rx_other_paid7['opioid_claim_rank'] = rx_other_paid7.groupby('id')['Days'].rank(method='first', ascending=False)\n",
    "rx_other_paid7 = rx_other_paid7[rx_other_paid7['opioid_claim_rank'] == 1]\n",
    "rx_other_paid7.set_index('id',inplace=True)\n",
    "\n",
    "# Sum of all amounts\n",
    "rx_other_paid8 = rx_other_paid.copy(deep=True)\n",
    "rx_other_paid8['event_attr3'] = rx_other_paid8['event_attr3'].map(to_float)\n",
    "rx_other_paid8['event_attr4'] = rx_other_paid8['event_attr4'].map(to_float)\n",
    "rx_other_paid8.fillna(0, inplace=True)\n",
    "rx_other_paid9 = rx_other_paid8.groupby(['id'])[['event_attr3', 'event_attr4', 'event_attr9']].sum()\n",
    "rx_other_paid9.columns = ['tot_rx_cost', 'tot_net_paid_amt', 'tot_mem_res_amt']\n",
    "rx_other_paid9.reset_index(inplace=True)\n",
    "rx_other_paid9.set_index('id',inplace=True)\n",
    "\n",
    "\n",
    "# Rejected claims\n",
    "rx_rejected = rx[(rx['event_descr'] == 'RX Claim - Rejected')&(rx['Days']<0)]\n",
    "\n",
    "# Total rejected claim count\n",
    "rx_rejected1 = pd.DataFrame(rx_rejected.groupby('id')['id'].count())\n",
    "rx_rejected1.columns = ['rejected_claim_count']\n",
    "rx_rejected1.reset_index(inplace=True)\n",
    "rx_rejected1.set_index('id',inplace=True)\n",
    "\n",
    "\n",
    "# Total rejected claim count by reason\n",
    "rx_rejected2 = pd.DataFrame(rx_rejected.groupby(['id', 'event_attr1'])['id'].count())\n",
    "rx_rejected2.columns = ['reason_rejected_claim_count']\n",
    "rx_rejected2.reset_index(inplace = True)\n",
    "rx_rejected3 = pd.DataFrame(pd.pivot_table(rx_rejected2, index = 'id', columns = 'event_attr1', \n",
    "                                           values = 'reason_rejected_claim_count')).reset_index()\n",
    "rx_rejected3.fillna(0, inplace=True)\n",
    "\n",
    "# Total first mail order claim count\n",
    "rx_first_mail = rx[rx['event_descr'] == 'RX Claim - First Time Mail Order']\n",
    "rx_first_mail1 = pd.DataFrame(rx_first_mail.groupby('id')['id'].count())\n",
    "rx_first_mail1.columns = ['first_mail_claim_count']\n",
    "rx_first_mail1.reset_index(inplace=True)\n",
    "rx_first_mail1.set_index('id',inplace=True)\n",
    "\n",
    "\n",
    "# rx_first_mail1[:10]\n",
    "# rx_other_paid2\n",
    "rx_other_paid5[:2]\n",
    "# rx_other_paid7[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rx_other_paid9[rx_other_paid9.isnull()])\n",
    "rx_other_paid9[rx_other_paid9.isnull()]\n",
    "# ID10010854159\n",
    "# rx_other_paid9[rx_other_paid9['id']=='ID10010854159']\n",
    "# rx_other_paid9.columns\n",
    "# rx_other_paid9['tot_rx_cost'].isnull()#[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# ------- Feature matrix creation\n",
    "feature_mat0 = all_id.join(latest_dx_id, on='id', how='left')\n",
    "feature_mat1 = feature_mat0.join(latest_new_dx_id, on='id',how='left')\n",
    "feature_mat2 = feature_mat1.join(feature_dx1, on='id',how='left')\n",
    "feature_mat3 = feature_mat2.join(latest_new_prov, on='id',how='left')\n",
    "feature_mat4 = feature_mat3.join(feature_data_call, on='id',how='left')\n",
    "feature_mat5 = feature_mat4.join(id_op_mat, on='id',how='left')\n",
    "feature_mat6 = feature_mat5.join(latest_sx_id, on='id',how='left')\n",
    "feature_mat7 = feature_mat6.join(rx_other_paid7['claim_end_day'], on='id',how='left')\n",
    "feature_mat8 = feature_mat7.join(rx_other_paid2, on='id',how='left')\n",
    "feature_mat9 = feature_mat8.join(rx_rejected1, on='id',how='left')\n",
    "feature_mat = feature_mat9.join(rx_other_paid9, on='id',how='left')\n",
    "\n",
    "feature_mat['no_new_dx_flag'] = np.where(feature_mat['latest_new_dx_day'].isnull(),1,0) \n",
    "feature_mat['no_new_sx_flag'] = np.where(feature_mat['Days_last_sx'].isnull(),1,0)\n",
    "feature_mat['on_op_pre'] = np.where(~feature_mat['actual_dos'].isnull(),1,0)\n",
    "feature_mat['Days_last_sx_bkt'] = pd.qcut(feature_mat['Days_last_sx'].values, 5).codes\n",
    "feature_mat['latest_new_dx_day_bkt'] = pd.qcut(feature_mat['latest_new_dx_day'].values, 5).codes\n",
    "feature_mat['claim_end_day_bkt'] = pd.qcut(feature_mat['claim_end_day'].values, 5).codes\n",
    "feature_mat['actual_dos_bkt'] = pd.qcut(feature_mat['actual_dos'].values, 5).codes\n",
    "feature_mat['rejection_ratio'] = 1*(feature_mat['rejected_claim_count'])/(feature_mat['Opioid Treatment']+feature_mat['Other Treatment']+feature_mat['Pain Treatment'])\n",
    "# feature_mat['Days_last_dx_bkt'] = pd.qcut(feature_mat['Days_last_dx'].values, 5).codes\n",
    "feature_mat['net_mbr_res_amt'] = feature_mat['mbr_res_amt']+feature_mat['mem_res_cost']+feature_mat['tot_mem_res_amt']\n",
    "feature_mat['op_rx_cost_share'] = feature_mat['mem_res_cost']/feature_mat['tot_mem_res_amt']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bins = np.array([0,5,25,50,150,250,1000,5000,10000])\n",
    "values = {'new_dx_CPD':0,'new_dx_hypertension': 0,'new_dx_top5': 0,'new_dx_CAD': 0,'new_dx_diabetes':0,\n",
    "          'new_dx_CHF':0,'latest_new_dx_day':0,'latest_sx_desc':'unk','latest_sx_pot':'unk',\n",
    "          'Days_last_sx':0,'latest_new_prov_date':0,'call_by_mbr':0,'call_by_other':0,'call_by_prov':0,\n",
    "         'actual_dos':0,'PAY_DAY_SUPPLY_CNT':0,'PAYABLE_QTY':0,'total_rx_MME':0,'rx_cost':0,'mem_res_cost':0,\n",
    "         'claim_end_day':0,'Opioid Treatment':0,'Other Treatment':0,'Pain Treatment':0,'rejected_claim_count':0,\n",
    "         'rejection_ratio':0,'latest_dx_desc':'unk','latest_dx_pot':'unk','Days_last_dx':0,\n",
    "         'tot_charge_amt':0,'net_paid_amt':0,'mbr_res_amt':0}\n",
    "\n",
    "feature_mat = feature_mat.fillna(value=values)\n",
    "feature_mat.rename(columns={'actual_dos':'actual_dos_op_pre','PAY_DAY_SUPPLY_CNT':'dos_op_pre',\n",
    "                           'PAYABLE_QTY':'op_qty_pre','total_rx_MME':'MME','rx_cost':'tot_rx_cost_op',\n",
    "                           'mem_res_cost':'mem_res_cost_op','claim_end_day':'last_op_claim_end_day',\n",
    "                           'Opioid Treatment':'claims_op_pre','Other Treatment':'claims_oth_pre',\n",
    "                           'Pain Treatment':'claims_pain_pre','actual_dos_bkt':'actual_dos_op_pre_bkt',\n",
    "                           'tot_charge_amt':'dx_tot_charge_amt','net_paid_amt':'dx_net_paid_amt',\n",
    "                           'mbr_res_amt':'dx_mbr_res_amt','claim_end_day_bkt':'last_op_claim_end_day_bkt'},inplace=True)\n",
    "\n",
    "print(\"Column-wise missing value percentage\")\n",
    "print(feature_mat.isnull().sum()/len(feature_mat)*100)\n",
    "# feature_mat.describe()\n",
    "# feature_mat['LTOT_flag'].sum()/len(feature_mat)*100\n",
    "\n",
    "\n",
    "feature_mat.plot.scatter(x='dx_mbr_res_amt',y='dx_tot_charge_amt',c='LTOT_flag',colormap='viridis',sharex=False)\n",
    "xlabel('mbr_res_amt')\n",
    "ylabel('tot_charge_amt')\n",
    "show()\n",
    "\n",
    "# mbr_res_amt\n",
    "sc_var = ['claims_op_pre','claims_oth_pre','claims_pain_pre']\n",
    "# sc_var = ['actual_dos_op_pre','dos_op_pre','op_qty_pre','MME','tot_rx_cost_op','mem_res_cost_op','mbr_res_amt','last_op_claim_end_day']\n",
    "\n",
    "corr = feature_mat[sc_var].corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=90,\n",
    "    horizontalalignment='right'\n",
    "#     ax.set_xticks(np.arange(15)\n",
    ");\n",
    "show()\n",
    "\n",
    "# ft = feature_mat[feature_mat['no_new_sx_flag']==0]['Days_last_sx'].hist()\n",
    "# show()\n",
    "\n",
    "# feature_mat[['last_op_claim_end_day','last_op_claim_end_day_bkt']].groupby('last_op_claim_end_day_bkt').describe()\n",
    "# # feature_mat.iloc[1]\n",
    "\n",
    "# df = ft\n",
    "# # plt.subplots(2, 2, figsize=(8,6))\n",
    "# def sephist(col):\n",
    "#     LTOT = df[df['LTOT_flag'] == 1][col]\n",
    "#     non_LTOT = df[df['LTOT_flag'] == 0][col]\n",
    "#     return LTOT, non_LTOT\n",
    "\n",
    "# for num, Days_last_sx in enumerate(ft[['Days_last_sx','LTOT_flag']]):\n",
    "#     plt.subplot(2, 2, num)\n",
    "#     plt.hist(sephist('Days_last_sx')[0], bins=25, alpha=0.5, label='LTOT', color='b')\n",
    "#     plt.hist(sephist('Days_last_sx')[1], bins=25, alpha=0.5, label='non_LTOT', color='r')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.title(alpha)\n",
    "# plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "# sephist('Days_last_sx')[0]\n",
    "# plt.subplot(2, 2)\n",
    "# bins=np.linspace(ft['Days_last_sx'].min(), ft['Days_last_sx'].max(), 10)\n",
    "# g = sns.FacetGrid(ft['Days_last_sx'], col=\"LTOT_flag\", hue=\"LTOT_flag\", palette=\"Set1\", col_wrap=2)\n",
    "# g.map(plt.hist, ft['Days_last_sx'], bins=bins, ec=\"k\")\n",
    "\n",
    "# g.axes[-1].legend()\n",
    "# plt.show()\n",
    "\n",
    "# print ('done')\n",
    "\n",
    "# feature_mat['Days_last_sx_bkt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier as sgd\n",
    "from sklearn.model_selection import cross_val_score as cvs\n",
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, average_precision_score, precision_recall_curve, roc_curve,roc_auc_score,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from inspect import signature\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = np.linspace(-12570,25775,1000)\n",
    "# threshold\n",
    "# alphas =  10**np.linspace(10,-2,100)*0.5\n",
    "# alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = feature_mat#[:1000]\n",
    "\n",
    "x_var = ['no_new_dx_flag','no_new_sx_flag',\n",
    "         'latest_new_dx_day_bkt','new_dx_CPD','new_dx_hypertension','new_dx_CAD','new_dx_diabetes','new_dx_CHF',\n",
    "         'dx_mbr_res_amt', 'call_by_mbr',\n",
    "         'Days_last_sx_bkt', 'Days_last_dx_bkt',\n",
    "         'actual_dos_op_pre_bkt','on_op_pre','MME','mem_res_cost_op','last_op_claim_end_day_bkt',\n",
    "        'claims_pain_pre','claims_oth_pre','claims_op_pre',\n",
    "        'rejection_ratio']\n",
    "# new_dx_top5\n",
    "X = dataset[x_var]\n",
    "Y = dataset['LTOT_flag']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.40, random_state=15)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Don't cheat - fit only on training data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)  # apply same transformation to test data\n",
    "# print ('Train:',len(X_train),' Test:',len(X_test))\n",
    "Y_test.describe()\n",
    "\n",
    "\n",
    "################### Baseline classifier\n",
    "\n",
    "class base_classifier(BaseEstimator):\n",
    "    def fit(self, X, Y):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X),1), dtype=bool)\n",
    "\n",
    "base_clf = base_classifier()\n",
    "base_accuracy = cvs(base_clf, X, Y, cv=3, scoring='accuracy')\n",
    "\n",
    "\n",
    "# ################### Building SGD Classifier\n",
    "\n",
    "sgd_clf = sgd(alpha=0.0001, average=False, class_weight=None,\n",
    "           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "           l1_ratio=0.3, learning_rate='optimal', loss='modified_huber', max_iter=10,\n",
    "           n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
    "           random_state=20, shuffle=True, tol=0.001,\n",
    "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "\n",
    "sgd_model = sgd_clf.fit(X_train,Y_train)\n",
    "sgd_accuracy = cvs(sgd_clf, X_test, Y_test, cv=3, scoring='accuracy')\n",
    "y_test_pred = cvp(sgd_clf, X_test, Y_test, cv=3)\n",
    "y_scores = cvp(sgd_clf, X_test, Y_test, cv=3,method='decision_function')\n",
    "fpr, tpr, th = roc_curve(Y_test,y_scores)\n",
    "sgd_auc = roc_auc_score(Y_test, y_test_pred)\n",
    "\n",
    "\n",
    "# -------------RandomForest\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=1000, criterion=\"entropy\", max_depth=None, \n",
    "                                min_samples_split=3, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                max_features=\"auto\", max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None,\n",
    "                                random_state=40, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "rf_model = rf_clf.fit(X_train,Y_train)\n",
    "rf_accuracy_train = cvs(rf_model, X_train, Y_train, cv=3, scoring='accuracy')\n",
    "rf_accuracy = cvs(rf_model, X_test, Y_test, cv=3, scoring='accuracy')\n",
    "y_test_pred_rf = cvp(rf_model, X_test, Y_test, cv=3)\n",
    "y_prob_forest = cvp(rf_model, X_test, Y_test, cv=3,method='predict_proba')\n",
    "y_scores_forest = y_prob_forest[:,1]\n",
    "fpr_rf, tpr_rf, th_rf = roc_curve(Y_test,y_scores_forest)\n",
    "rf_auc = roc_auc_score(Y_test, y_test_pred_rf)\n",
    "\n",
    "# ---------------RandomForest on entire data\n",
    "rf_model_all = rf_clf.fit(X,Y)\n",
    "rf_accuracy_all = cvs(rf_model_all, X, Y, cv=3, scoring='accuracy')\n",
    "y_all_pred_rf = cvp(rf_model_all, X, Y, cv=3)\n",
    "y_prob_forest_all = cvp(rf_model_all, X, Y, cv=3,method='predict_proba')\n",
    "y_scores_forest_all = y_prob_forest_all[:,1]\n",
    "fpr_rfall, tpr_rfall, th_rfall = roc_curve(Y,y_scores_forest_all)\n",
    "rf_auc_all = roc_auc_score(Y, y_all_pred_rf)\n",
    "\n",
    "# ------------------GradientBoost\n",
    "gbm_clf = GradientBoostingClassifier(loss=\"deviance\", learning_rate=0.1, n_estimators=1000, \n",
    "                                     subsample=1.0, criterion=\"friedman_mse\", min_samples_split=2, \n",
    "                                     min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "                                     min_impurity_decrease=0.0, min_impurity_split=None, init=None,\n",
    "                                     random_state=None, max_features=None, verbose=0, max_leaf_nodes=None,\n",
    "                                     warm_start=False, presort=\"auto\", validation_fraction=0.1,\n",
    "                                     n_iter_no_change=None, tol=0.0001)\n",
    "\n",
    "# gbm_model = gbm_clf.fit(X_train,Y_train)\n",
    "# gbm_accuracy_train = cvs(gbm_model, X_train, Y_train, cv=3, scoring='accuracy')\n",
    "# gbm_accuracy_test = cvs(gbm_model, X_test, Y_test, cv=3, scoring='accuracy')\n",
    "# y_test_pred_gbm = cvp(gbm_model, X_test, Y_test, cv=3)\n",
    "# y_test_prob_gbm = cvp(gbm_model, X_test, Y_test, cv=3,method='predict_proba')\n",
    "# y_test_scores_gbm = y_test_prob_gbm[:,1]\n",
    "# fpr_gbm, tpr_gbm, th_gbm = roc_curve(Y_test,y_test_scores_gbm)\n",
    "# gbm_auc = roc_auc_score(Y_test, y_test_pred_gbm)\n",
    "\n",
    "gbm_model = gbm_clf.fit(X,Y)\n",
    "gbm_accuracy_train = cvs(gbm_model, X, Y, cv=3, scoring='accuracy')\n",
    "gbm_accuracy_test = cvs(gbm_model, X, Y, cv=3, scoring='accuracy')\n",
    "y_test_pred_gbm = cvp(gbm_model, X, Y, cv=3)\n",
    "y_test_prob_gbm = cvp(gbm_model, X, Y, cv=3,method='predict_proba')\n",
    "y_test_scores_gbm = y_test_prob_gbm[:,1]\n",
    "fpr_gbm, tpr_gbm, th_gbm = roc_curve(Y,y_test_scores_gbm)\n",
    "gbm_auc = roc_auc_score(Y, y_test_pred_gbm)\n",
    "\n",
    "\n",
    "# ---------ROC curve\n",
    "def plot_roc_curve(fpr,tpr,label=None):\n",
    "    plt.plot(fpr,tpr,linewidth=2,label=label)\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.xlabel('False Positive Rate : FP/FP+TN')\n",
    "    plt.ylabel('True Positive Rate : TP/TP+FN')\n",
    "    \n",
    "plt.plot(fpr,tpr,\"b:\",label=\"SGD\")\n",
    "# plot_roc_curve(fpr_rf,tpr_rf,\"RF\")\n",
    "plot_roc_curve(fpr_rfall,tpr_rfall,\"RF-all\")\n",
    "plot_roc_curve(fpr_gbm,tpr_gbm,\"GBM test\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# ----------- model evaluation\n",
    "print ('rf accuracy all (TP+TN/n): ',rf_accuracy_all.mean(),'|rf accuracy Train: ',rf_accuracy_train.mean(),'|rf accuracy Test: ',rf_accuracy.mean())\n",
    "# print ('rf accuracy (TP+TN/n): ',rf_accuracy.mean())\n",
    "print ('sgd accuracy (TP+TN/n: ',sgd_accuracy.mean())\n",
    "print ('base accuracy (TP+TN/n: ',base_accuracy.mean())\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "cm_rf = confusion_matrix(Y_test, y_test_pred_rf)\n",
    "cm_rf_all = confusion_matrix(Y, y_all_pred_rf)\n",
    "index = ['NON LTOT','LTOT']  \n",
    "columns = ['NON LTOT','LTOT']    \n",
    "cm_df = pd.DataFrame(cm,columns,index)\n",
    "cm_df_rf = pd.DataFrame(cm_rf,columns,index)\n",
    "# print ('SGD')\n",
    "# plt.figure(figsize=(5,3))  \n",
    "# sns.heatmap(cm_df,annot=True,fmt='g')\n",
    "# show()\n",
    "# print ('RF')\n",
    "# plt.figure(figsize=(5,3))  \n",
    "# sns.heatmap(cm_rf,annot=True,fmt='g')\n",
    "# show()\n",
    "print ('RF all')\n",
    "plt.figure(figsize=(5,3))  \n",
    "sns.heatmap(cm_rf_all,annot=True,fmt='g')\n",
    "show()\n",
    "print(classification_report(Y, y_all_pred_rf)) ##, target_names=target_names\n",
    "\n",
    "cm_gbm = confusion_matrix(Y, y_test_pred_gbm)\n",
    "print('GBM')\n",
    "plt.figure(figsize=(5,3))  \n",
    "sns.heatmap(cm_gbm,annot=True,fmt='g')\n",
    "show()\n",
    "\n",
    "print ('gbm accuracy Train: ',gbm_accuracy_train.mean(),'|gbm accuracy Test: ',gbm_accuracy_test.mean())\n",
    "print(classification_report(Y, y_test_pred_gbm)) ##, target_names=target_names\n",
    "print ('gbm_auc=',gbm_auc)\n",
    "print('sgd_auc=',sgd_auc)\n",
    "# print('rf_auc=',rf_auc)\n",
    "print('rf_auc_all=',rf_auc_all)\n",
    "\n",
    "feature_importances = pd.DataFrame(rf_model_all.feature_importances_,\n",
    "                                   index = x_var,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "print (feature_importances)\n",
    "\n",
    "# ---------summary final model on entire data \n",
    "df = dataset\n",
    "df['y_pred_rf'] = y_all_pred_rf\n",
    "df['y_pred_gbm'] = y_test_pred_gbm\n",
    "\n",
    "def create_label(row,col):\n",
    "    if row['LTOT_flag'] == row[col] and row[col]==1:\n",
    "        return 'TP'\n",
    "    elif row['LTOT_flag'] == row[col] and row[col]==0:\n",
    "        return 'TN'\n",
    "    elif row['LTOT_flag'] != row[col] and row[col]==0:\n",
    "        return 'FN'\n",
    "    else:\n",
    "        return 'FP'\n",
    "\n",
    "df['label_rf'] = df.apply (lambda row: create_label(row,'y_pred_rf'), axis=1)\n",
    "df['label_gbm'] = df.apply (lambda row: create_label(row,'y_pred_gbm'), axis=1)\n",
    "\n",
    "\n",
    "# summary_var = ['label','dos_op_pre','MME']\n",
    "# claims_oth_pre           0.101260\n",
    "# rejected_claim_count     0.091630\n",
    "# MME                      0.090429\n",
    "# call_by_mbr              0.080635\n",
    "# mem_res_cost_op          0.066694\n",
    "# last_op_claim_end_day    0.060812\n",
    "# claims_op_pre            0.058836\n",
    "# Days_last_sx_bkt         0.037470\n",
    "# latest_new_dx_day_bkt    0.025136\n",
    "# no_new_sx_flag           0.008547\n",
    "# on_op_pre                0.007803\n",
    "# new_dx_CPD               0.007772\n",
    "# new_dx_hypertension      0.007461\n",
    "# no_new_dx_flag           0.007335\n",
    "# new_dx_diabetes          0.006155\n",
    "# new_dx_CAD               0.005924\n",
    "# new_dx_CHF               0.005332\n",
    "# df[summary_var].groupby('label').describe()\n",
    "\n",
    "# df.iloc[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df1 = scaler.transform(df[['dos_op_pre','MME']])\n",
    "# df1['label_rf'] = df['label_rf']\n",
    "\n",
    "#set font size of labels on matplotlib plots\n",
    "plt.rc('font', size=12)\n",
    "# plt.figure(figsize=(5,3)) \n",
    "\n",
    "#set style of plots\n",
    "sns.set_style('white')\n",
    "\n",
    "#define a custom palette\n",
    "customPalette = ['#630C3A', '#39C8C6', '#D3500C', '#FFB139']\n",
    "sns.set_palette(customPalette)\n",
    "sns.palplot(customPalette)\n",
    "\n",
    "#plot data with seaborn\n",
    "# plt.figure(figsize=(5,3)) \n",
    "facet = sns.lmplot(data=df, x='rejection_ratio', y='dos_op_pre', hue='label_rf', \n",
    "                   fit_reg=False, legend=True, legend_out=True)\n",
    "\n",
    "\n",
    "\n",
    "# df.plot.scatter(x='dos_op_pre',y='mbr_res_amt',c='label_rf',label='label_rf', colormap='viridis',sharex=False)\n",
    "# xlabel('dos_op_pre')\n",
    "# ylabel('mbr_res_amt')\n",
    "# show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------GradientBoost\n",
    "gbm_clf = GradientBoostingClassifier(loss=\"deviance\", learning_rate=0.1, n_estimators=1000, \n",
    "                                     subsample=1.0, criterion=\"friedman_mse\", min_samples_split=2, \n",
    "                                     min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "                                     min_impurity_decrease=0.0, min_impurity_split=None, init=None,\n",
    "                                     random_state=None, max_features=None, verbose=0, max_leaf_nodes=None,\n",
    "                                     warm_start=False, presort=\"auto\", validation_fraction=0.1,\n",
    "                                     n_iter_no_change=None, tol=0.0001)\n",
    "\n",
    "gbm_model = gbm_clf.fit(X_train,Y_train)\n",
    "gbm_accuracy_train = cvs(gbm_model, X_train, Y_train, cv=3, scoring='accuracy')\n",
    "gbm_accuracy_test = cvs(gbm_model, X_test, Y_test, cv=3, scoring='accuracy')\n",
    "y_test_pred_gbm = cvp(gbm_model, X_test, Y_test, cv=3)\n",
    "y_test_prob_gbm = cvp(gbm_model, X_test, Y_test, cv=3,method='predict_proba')\n",
    "y_test_scores_gbm = y_test_prob_gbm[:,1]\n",
    "fpr_gbm, tpr_gbm, th_gbm = roc_curve(Y_test,y_test_scores_gbm)\n",
    "gbm_auc = roc_auc_score(Y_test, y_test_pred_gbm)\n",
    "\n",
    "# ----------model summary\n",
    "cm_gbm = confusion_matrix(Y_test, y_test_pred_gbm)\n",
    "plt.figure(figsize=(5,3))  \n",
    "sns.heatmap(cm_gbm,annot=True,fmt='g')\n",
    "show()\n",
    "\n",
    "plt.plot(fpr_gbm,tpr_gbm,\"b:\",label=\"GBM\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print ('rf accuracy Train: ',gbm_accuracy_train.mean(),'|rf accuracy Test: ',gbm_accuracy_test.mean())\n",
    "print(classification_report(Y_test, y_test_pred_gbm)) ##, target_names=target_names\n",
    "print ('gbm_auc=',gbm_auc)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opioid Addiction:\n",
    "As part of the national effort to combat opioid overuse, Humana set a goal to reduce the number of members receiving opioid prescriptions greater than 100 morphine milligram equivalent (MME), a dosage that raises the risks of opioid overdose, by 40 percent. In 2018, Humana closed in on its goal, reducing the number of members receiving prescriptions greater than 100 MME by 36 percent.\n",
    "\n",
    "# Diagnosis data with LTOT flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_LTOT = rx_op_episode.groupby('id')['LTOT_flag'].max()#.rename(columns={'id':'LTOT_flag'})\n",
    "print (dx.shape)\n",
    "dx_p1 = dx.merge(pat_LTOT,\n",
    "               left_on='id',\n",
    "               right_on='id',\n",
    "               how='left',\n",
    "               suffixes=['_dx', '_LTOT'])\n",
    "aggregation = {'Days':min,'Days':max}        \n",
    "dx_p3 = dx_p1[dx_p1['event_descr']=='Fully Paid Claim'].groupby(['id','LTOT_flag','event_attr1'])['Days'].count()\n",
    "dx_p3.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# dx_p1[dx_p1['id']=='ID10013863216']\n",
    "# # pat_LTOT\n",
    "# dx_p2 = dx_p1[dx_p1['event_descr']=='Fully Paid Claim'].groupby(['id','LTOT_flag','event_attr1'])['Days'].count()\n",
    "# dx_p2.reset_index(inplace=True)\n",
    "# # dx_p2 = pd.DataFrame({'id':dx_p2.index, 'count':dx_p2.values})\n",
    "# dx_p2\n",
    "\n",
    "# # dx_p2['diagnosis_rank_by_count'] = dx_p2['Days']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rx_paid_claim_pat[rx_paid_claim_pat['count']>1][:5]\n",
    "rx_paid_claim_nn = rx_paid_claim.loc[pd.notnull(rx_paid_claim.MME)]\n",
    "rx_paid_claim_pat_1 = rx_paid_claim_nn.groupby(['id'])[['event_descr']].count().rename(columns={'event_descr':'count'})\n",
    "#rx_paid_claim_pat_1['count'].value_counts().plot(kind='bar')\n",
    "\n",
    "print ('Drug class for qualifying events (Naive opioid patients')\n",
    "print (rx_paid_claim_nn['event_attr1'].value_counts())\n",
    "\n",
    "# rx_paid_claim_nn[:5]\n",
    "\n",
    "# print (rx_op_episode.shape)    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# aggregation = {'episode_start_day':{'episode_start_day':min},'episode_end_day':{'episode_end_day':max},'actual_dos':{'episode_len':sum}}        \n",
    "#     rx_op_claim.loc[index,'episode_num'] = rx_op_claim.loc[index,'episode_rank']\n",
    "#     if n>1: \n",
    "#         if (rx_op_claim.loc[n,'id']==rx_op_claim.loc[n-1,'id'])&(rx_op_claim.loc[n,'episode_rank']==1):\n",
    "#             if(rx_op_claim.loc[n,'is_episode_start']==0):\n",
    "#                 rx_op_claim.loc[n,'episode_num'] = rx_op_claim.loc[n-1,'episode_rank']\n",
    "#             else:\n",
    "#                 rx_op_claim.loc[n,'episode_num'] = rx_op_claim.loc[n,'episode_rank']\n",
    "#         else:\n",
    "#             rx_op_claim.loc[n,'episode_num'] = rx_op_claim.loc[n,'episode_rank']\n",
    "#     else:\n",
    "#         rx_op_claim.loc[n,'episode_num'] = 1\n",
    "\n",
    "\n",
    "# rx_op_claim['episode_num'] = rx_op_claim.apply(lambda x:episode_num(x),axis=1)     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rx_op_claim_1 = rx_op_claim[rx_op_claim['is_episode_start']==1]\n",
    "# rx_op_claim_1['episode_number'] = rx_op_claim_1['episode_start_day'].rank(method='dense')\n",
    "\n",
    "\n",
    "# grace = 0\n",
    "# rx_op_claim['continuing'] = np.where(rx_op_claim[''])\n",
    "# rx_naive_op_claim['days_on_op'] = np.where(rx_naive_op_claim['gap_to_next_op']<rx_naive_op_claim['PAY_DAY_SUPPLY_CNT'],rx_naive_op_claim['gap_to_next_op'],rx_naive_op_claim['PAY_DAY_SUPPLY_CNT'])\n",
    "# rx_naive_op_claim['in_next_180_days'] = np.where((rx_naive_op_claim['Days']>=0)&(rx_naive_op_claim['Days']<180),1,0)\n",
    "# rx_LTOT_1 = rx_naive_op_claim[(rx_naive_op_claim['Days']>=0)&(rx_naive_op_claim['Days']<180)]\n",
    "# rx_LTOT_2 = rx_LTOT_1.groupby(['id'])[['days_on_op']].sum()\n",
    "# rx_LTOT_2.reset_index(inplace=True)\n",
    "# rx_LTOT_pat = rx_LTOT_2[rx_LTOT_2['days_on_op']>=162]\n",
    "# rx_LTOT_pat#['id'].value_counts()\n",
    "# # rx_naive_claims_pat[rx_naive_claims_pat['sum']>2]\n",
    "# [rx_op_claim['is_episode_start']==1]\n",
    "\n",
    "# rx_op_claim_1 = rx_op_claim_1[rx_op_claim_1['id']=='ID37774657894']\n",
    "rx_op_claim#.drop(drop_col,axis=1)\n",
    "\n",
    "# rx_op_episode.reset_index(inplace=True)\n",
    "# print (rx_op_episode)\n",
    "\n",
    "rx_op_episode\n",
    "# rx_op_claim\n",
    "# rx_op_episode_180_1.sort_values(by='days_on_op_next_180',ascending=False)\n",
    "rx_op_episode_180_1#['days_on_op_next_180'].hist()\n",
    "rx_op_episode\n",
    "# rx_op_claim[['id','Days','PAY_DAY_SUPPLY_CNT','actual_dos','claim_end_day','next_op']]\n",
    "# rx_naive_op_claim[rx_naive_op_claim['Days']<0].max()\n",
    "# rx_test = rx_naive_op_claim[rx_naive_op_claim['id']=='ID99994197161']\n",
    "\n",
    "\n",
    "\n",
    "# rx_LTOT_1[rx_LTOT_1['id']=='ID37774657894'][col]\n",
    "# # rx_LTOT_1[rx_LTOT_1['id']=='ID37774657894'][col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rx[~rx['MME'].isnull()].iloc[1]\n",
    "# print ('event_attr6=Drug group description')\n",
    "# print (rx[rx['MME'].isnull()]['event_attr6'].value_counts())\n",
    "# # ## \n",
    "# print ('event_attr8=Generic name')\n",
    "# rx[rx['MME'].isnull()]['event_attr8'].value_counts()#/rx[rx['MME'].isnull()]['event_attr8'].value_counts().sum()*100\n",
    "\n",
    "\n",
    "# rx[~rx['MME'].isnull()]['MME'].hist(bins=[0,100,200,300,400,500,600])#.value_counts()[10]#.plot(kind='bar')\n",
    "# rx[~rx['MME'].isnull()]['MME'].max()\n",
    "# rx[rx['MME']==1872]\n",
    "# show()\n",
    "\n",
    "# rx_LTOT_2[rx_LTOT_2['id']=='ID10024447278']\n",
    "\n",
    "# rx_LTOT_2\n",
    "\n",
    "#.loc[1]\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df = df[df['PRICE'] == df.groupby('ORIGIN')['PRICE'].transform('min')]\n",
    "\n",
    "\n",
    "# rx_naive_op_claim.iloc[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rx_test['previous_op'] = rx_test['Days'].shift(1) \n",
    "\n",
    "# rx_test[\"claim_no\"] = rx_test.groupby(\"id\")[\"Days\"].rank(\"dense\", ascending=True)\n",
    "# rx_test = rx_test[rx_test['PRICE'] != df.groupby('ORIGIN')['PRICE'].transform('min')]\n",
    "\n",
    "\n",
    "# rx_test_rnk = rx_test.groupby(['id'])[['claim_no']].sum().rename(columns={'Naive_opioid_claim':'sum'})\n",
    "\n",
    "\n",
    "# rx_test\n",
    "\n",
    "\n",
    "# rx_test = rx_test.loc[rx_test['claim_no'].first()]\n",
    "# # rx_test.set_index([['id','Days']])\n",
    "\n",
    "# rx_naive_op_claim[rx_naive_op_claim['id']=='ID37774657894'][]\n",
    "# rx[rx['id']=='ID10180352120'].loc[(rx['event_descr']=='RX Claim - Paid')&(rx['Days']==0)&(~rx['MME'].isnull())]\n",
    "# rx[rx['id']=='ID10180352120'].loc[(rx['event_descr']=='RX Claim - Paid')&(rx['Days']>0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = subplots(nrows=1,ncols=2)\n",
    "\n",
    "print ('Pre initiation')\n",
    "pre_dx = data_train[(data_train['Days']<0) & (data_train['event_category']=='Diagnosis')]['event_descr'].value_counts()\n",
    "pre_dx.plot(kind='bar')\n",
    "show()\n",
    "\n",
    "print ('Post initiation')\n",
    "post_dx = data_train[(data_train['Days']>0) & (data_train['event_category']=='Diagnosis')]['event_descr'].value_counts()\n",
    "post_dx.plot(kind='bar')\n",
    "show()\n",
    "\n",
    "print ('What is New diagnosis - Top 5')\n",
    "\n",
    "\n",
    "# ############################\n",
    "# Day zero analysis\n",
    "\n",
    "init = data_train[data_train['Days']==0].drop_duplicates()\n",
    "init['id'].value_counts()[:10].plot(kind='bar')\n",
    "show()\n",
    "print ('patients present with multiple initiation event date')\n",
    "\n",
    "data_train[(data_train['id']=='ID66796083593')&(data_train['Days']==0)&(data_train['event_descr']=='RX Claim - Paid')].sort_values(by='Days')\n",
    "\n",
    "print('For day 0 events we see top two categories of claims as per event attr1 to be related to opioids')\n",
    "data_train[(data_train['Days']==0)&(data_train['event_descr']=='RX Claim - Paid')]['event_attr1'].value_counts()[:10].plot(kind='bar')\n",
    "show()\n",
    "\n",
    "data_rx_train = data_train[data_train['event_category']=='RxClaim']\n",
    "data_rx_train['event_descr'].value_counts().plot(kind='bar')\n",
    "show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
